{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EU crop map - Benchmarking on the indices for the crops classes\n",
    "## 1) Split by polygones - accuracy per pixels and per polygone\n",
    "## 2) Split by pixels - accuracy per pixels\n",
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JEODPP\n",
    "data_path='/eos/jeodpp/data/projects/REFOCUS/data/S1_GS/all-10days/Map_v7/'\n",
    "project_path='/eos/jeodpp/data/projects/REFOCUS/classification/'\n",
    "path_pol = '/eos/jeodpp/data/projects/REFOCUS/data/polygons/v7'\n",
    "results='/eos/jeodpp/data/projects/REFOCUS/classification/'\n",
    "\n",
    "local='/eos/jeodpp/home/users/verheas/data/LUCAS/v7/'\n",
    "\n",
    "#working directory\n",
    "pwd = project_path\n",
    "\n",
    "# !pip install matplotlib --user\n",
    "# !pip install sklearn --user\n",
    "#import \n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd_lucas (2956889, 116)\n",
      "300    1216530\n",
      "200    1000318\n",
      "500     732964\n",
      "600       3856\n",
      "100       3221\n",
      "Name: level_1, dtype: int64\n",
      "300    1216530\n",
      "500     732964\n",
      "211     290116\n",
      "213     142886\n",
      "216     125644\n",
      "232      64452\n",
      "290      63609\n",
      "250      59053\n",
      "214      35215\n",
      "231      34577\n",
      "240      34067\n",
      "215      34005\n",
      "212      28825\n",
      "222      23174\n",
      "218      19274\n",
      "221      15815\n",
      "230      12000\n",
      "233       7094\n",
      "223       4920\n",
      "219       4724\n",
      "600       3856\n",
      "100       3221\n",
      "217        868\n",
      "Name: level_2, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (27,30,33,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>POINT_ID</th>\n",
       "      <th>NUTS0</th>\n",
       "      <th>NUTS1</th>\n",
       "      <th>NUTS2</th>\n",
       "      <th>NUTS3</th>\n",
       "      <th>TH_LAT</th>\n",
       "      <th>TH_LONG</th>\n",
       "      <th>OFFICE_PI</th>\n",
       "      <th>EX_ANTE</th>\n",
       "      <th>...</th>\n",
       "      <th>LU2_LABEL</th>\n",
       "      <th>LU1_TYPE_LABEL</th>\n",
       "      <th>LU2_TYPE_LABEL</th>\n",
       "      <th>CPRN_LC_LABEL</th>\n",
       "      <th>CPRN_LC_SAME_LC1</th>\n",
       "      <th>LUCAS_CORE_INTERSECT</th>\n",
       "      <th>COPERNICUS_CLEANED</th>\n",
       "      <th>stratum</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34562080</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES2</td>\n",
       "      <td>ES24</td>\n",
       "      <td>ES243</td>\n",
       "      <td>41.288386</td>\n",
       "      <td>-0.319428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other bare soil</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>290</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31243520</td>\n",
       "      <td>IE</td>\n",
       "      <td>IE0</td>\n",
       "      <td>IE04</td>\n",
       "      <td>IE042</td>\n",
       "      <td>53.422977</td>\n",
       "      <td>-8.226052</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spontaneously re-vegetated surfaces</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>33661774</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES5</td>\n",
       "      <td>ES52</td>\n",
       "      <td>ES521</td>\n",
       "      <td>38.434388</td>\n",
       "      <td>-0.905705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Permanent crops: fruit trees</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>28922250</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES1</td>\n",
       "      <td>ES11</td>\n",
       "      <td>ES113</td>\n",
       "      <td>41.867145</td>\n",
       "      <td>-7.320304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shrubland with sparse tree cover</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>35082906</td>\n",
       "      <td>FR</td>\n",
       "      <td>FRD</td>\n",
       "      <td>FRD1</td>\n",
       "      <td>FRD12</td>\n",
       "      <td>48.715190</td>\n",
       "      <td>-1.092190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Grassland without tree/shrub cover</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  POINT_ID NUTS0 NUTS1 NUTS2  NUTS3     TH_LAT   TH_LONG  \\\n",
       "0           0  34562080    ES   ES2  ES24  ES243  41.288386 -0.319428   \n",
       "1           1  31243520    IE   IE0  IE04  IE042  53.422977 -8.226052   \n",
       "2           2  33661774    ES   ES5  ES52  ES521  38.434388 -0.905705   \n",
       "3           3  28922250    ES   ES1  ES11  ES113  41.867145 -7.320304   \n",
       "4           4  35082906    FR   FRD  FRD1  FRD12  48.715190 -1.092190   \n",
       "\n",
       "   OFFICE_PI  EX_ANTE  ...     LU2_LABEL  LU1_TYPE_LABEL  LU2_TYPE_LABEL  \\\n",
       "0          0        0  ...  Not relevant             NaN             NaN   \n",
       "1          0        0  ...  Not relevant             NaN             NaN   \n",
       "2          0        0  ...  Not relevant             NaN             NaN   \n",
       "3          0        0  ...  Not relevant             NaN             NaN   \n",
       "4          0        0  ...  Not relevant             NaN             NaN   \n",
       "\n",
       "                         CPRN_LC_LABEL  CPRN_LC_SAME_LC1  \\\n",
       "0                      Other bare soil              True   \n",
       "1  Spontaneously re-vegetated surfaces              True   \n",
       "2         Permanent crops: fruit trees              True   \n",
       "3     Shrubland with sparse tree cover              True   \n",
       "4   Grassland without tree/shrub cover              True   \n",
       "\n",
       "   LUCAS_CORE_INTERSECT  COPERNICUS_CLEANED  stratum  level_2  level_1  \n",
       "0                  True                True        2      290      200  \n",
       "1                  True                True        1      500      500  \n",
       "2                  True                True        2      300      300  \n",
       "3                  True                True        2      300      300  \n",
       "4                  True                True        1      500      500  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the data\n",
    "#1) load the S1 10 days extracted values in GEE for all polygons\n",
    "\n",
    "pd_lucas= pd.read_csv(os.path.join(data_path,'S1_point_allV7_10days_10m_1Jan-31Dec_EU_ratio-db.csv'),dtype={'level_1':int,'level_2':int})\n",
    "print('pd_lucas',pd_lucas.shape)\n",
    "\n",
    "#concatenate all the data in one dataframe\n",
    "#group cropland, grassland and bareland \n",
    "#number of pixels per class\n",
    "print(pd_lucas.level_1.value_counts())\n",
    "print(pd_lucas.level_2.value_counts())\n",
    "pd_lucas.head()\n",
    "\n",
    "#number of pixels per class\n",
    "#pd_lucas.LC1_COD.value_counts()\n",
    "#pd_lucas.head()\n",
    "pd_lucas.columns\n",
    "\n",
    "##############1.2 Load the shapefile with the polygons - useful to split the polygons in training and test dataset for the accuracy ######################\n",
    "# load csv with of the polygons\n",
    "#2)load csv with the polygons for the split test/validation\n",
    "lucas_polygons = pd.read_csv(os.path.join(path_pol,'LUCAS_2018_Copernicus_attributes_cropmap_level1-2.csv'))\n",
    "lucas_polygons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_B [211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 230, 231, 232, 233, 240, 250, 290]\n",
      "classes_NB []\n",
      "level level_2\n",
      "level [211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 230, 231, 232, 233, 240, 250, 290]\n",
      "[211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 230, 231, 232, 233, 240, 250, 290]\n",
      "    class                                 label\n",
      "36    230  other non permanent industrial crops\n",
      "37    240     dry pulse, vegetables and flowers\n",
      "38    250                    other fodder crops\n",
      "39    290                      bare arable land\n",
      "40    211                          common wheat\n",
      "41    212                           durum wheat\n",
      "42    213                                barley\n",
      "43    214                                   rye\n",
      "44    215                                  oats\n",
      "45    216                                 maize\n",
      "46    217                                  rice\n",
      "47    218                             triticale\n",
      "48    219                         other cereals\n",
      "49    221                              potatoes\n",
      "50    222                            sugar beet\n",
      "51    223                      other root crops\n",
      "52    231                             sunflower\n",
      "53    232                  rape and turnip rape\n",
      "54    233                                  soya\n",
      "84    290                      bare arable land\n",
      "    class                                 label\n",
      "36    230  other non permanent industrial crops\n",
      "37    240     dry pulse, vegetables and flowers\n",
      "38    250                    other fodder crops\n",
      "39    290                      bare arable land\n",
      "40    211                          common wheat\n",
      "41    212                           durum wheat\n",
      "42    213                                barley\n",
      "43    214                                   rye\n",
      "44    215                                  oats\n",
      "45    216                                 maize\n",
      "46    217                                  rice\n",
      "47    218                             triticale\n",
      "48    219                         other cereals\n",
      "49    221                              potatoes\n",
      "50    222                            sugar beet\n",
      "51    223                      other root crops\n",
      "52    231                             sunflower\n",
      "53    232                  rape and turnip rape\n",
      "54    233                                  soya\n",
      "84    290                      bare arable land\n"
     ]
    }
   ],
   "source": [
    "##################################Parameters##################################################\n",
    "#classes - stored in a table 'legend-lucas-all'\n",
    "table_class=pd.read_csv(os.path.join(project_path,'table/legend-lucas-all-v7.csv'),dtype=pd.Int64Dtype())\n",
    "\n",
    "classes_L1=list(table_class['classes_L1'].dropna()) \n",
    "classes_L2=list(table_class['classes_L2'].dropna())\n",
    "\n",
    "#level\n",
    "level_1='level_1'\n",
    "level_2='level_2'\n",
    "\n",
    "##################################Parameters##################################################\n",
    "#classes - stored in a table 'legend-lucas-all'\n",
    "table_class=pd.read_csv(os.path.join(project_path,'table/legend-lucas-all-v2.csv'),dtype=pd.Int64Dtype())\n",
    "\n",
    "classes_L1=list(table_class['classes_L1'].dropna())\n",
    "classes_L2=list(table_class['classes_L2'].dropna())\n",
    "\n",
    "#remap classes and selection of classes to map Level 1\n",
    "classes_in_L1 =  list(table_class['classes_all'].dropna())\n",
    "\n",
    "classes_in_L2 = list(table_class['classes_all'].dropna()),\n",
    "\n",
    "#classes affected by biome selection\n",
    "classes_L1_B= list(table_class['classes_L1_B'].dropna())\n",
    "classes_L2_B= list(table_class['classes_L2'].dropna())\n",
    "\n",
    "#Classes non affected by biome selection\n",
    "#Classes from (A) Artificial, (F) Bare lands and (H) Wetlands can be considered in each models - no biome dependent\n",
    "classes_L1_NB=list(table_class['classes_L1_NB'].dropna())\n",
    "classes_L2_NB=[]\n",
    "#summary of the classes used in the classification\n",
    "classes_classif_L1= list(table_class['L1_BIOME'].dropna())\n",
    "classes_classif_L1_simplify=list(table_class['L1_B_harmon'].dropna())\n",
    "\n",
    "#[100,200,520,300,400,600,800]\n",
    "classes_classif_L2=list(table_class['L2_BIOME'].dropna())\n",
    "classes_classif_L2_simplify=list(table_class['L2_B_harmon'].dropna())\n",
    "\n",
    "###################################Choose parameters for this run #############################################\n",
    "#classes for the classification and biome/no biome differentiation if needed\n",
    "classes_B=classes_L2_B\n",
    "print ('classes_B',classes_B)\n",
    "\n",
    "classes_NB=classes_L2_NB\n",
    "print ('classes_NB',classes_NB)\n",
    "\n",
    "#level\n",
    "level=level_2\n",
    "print('level',level)\n",
    "#crop - level 2, from the table we load only the crop type classes\n",
    "classes=classes_L2\n",
    "print('level',classes)\n",
    "\n",
    "###################################Labels of the classes #############################################\n",
    "labels_csv = pd.read_csv(os.path.join(project_path,'table/legend-lucas2.csv'))\n",
    "labels=labels_csv[labels_csv['class'].isin(classes)] # select only the used labels\n",
    "labels_s=labels_csv[labels_csv['class'].isin(classes)] # select only the used labels\n",
    "print(classes)\n",
    "print(labels)\n",
    "print(labels_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2956889, 117)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2111    261290\n",
      "2161    119697\n",
      "2131     98670\n",
      "2321     63899\n",
      "2132     44216\n",
      "2501     40453\n",
      "2901     35167\n",
      "2141     31956\n",
      "2112     28826\n",
      "2902     28442\n",
      "2151     25021\n",
      "2311     24369\n",
      "2221     22706\n",
      "2401     22440\n",
      "2502     18600\n",
      "2122     17962\n",
      "2181     17686\n",
      "2211     14965\n",
      "2402     11627\n",
      "2121     10863\n",
      "2312     10208\n",
      "2152      8984\n",
      "2301      7600\n",
      "2331      7020\n",
      "2162      5947\n",
      "2191      4625\n",
      "2302      4400\n",
      "2231      4036\n",
      "2142      3259\n",
      "2182      1588\n",
      "2232       884\n",
      "2212       850\n",
      "2171       661\n",
      "2322       553\n",
      "2222       468\n",
      "2172       207\n",
      "2192        99\n",
      "2332        74\n",
      "Name: ClassifB, dtype: int64\n",
      "(18376, 114)\n",
      "211    4829\n",
      "213    2545\n",
      "216    2383\n",
      "250    1332\n",
      "290    1312\n",
      "232    1111\n",
      "240     709\n",
      "231     684\n",
      "215     642\n",
      "214     603\n",
      "212     586\n",
      "222     403\n",
      "218     329\n",
      "221     308\n",
      "230     247\n",
      "233     155\n",
      "223      94\n",
      "219      90\n",
      "217      14\n",
      "Name: Classif, dtype: int64\n",
      "(0, 114)\n",
      "Series([], Name: Classif, dtype: int64)\n",
      "2111    4210\n",
      "2161    2242\n",
      "2131    1615\n",
      "2321    1096\n",
      "2132     930\n",
      "2501     860\n",
      "2902     683\n",
      "2901     629\n",
      "2112     619\n",
      "2141     528\n",
      "2502     472\n",
      "2311     462\n",
      "2151     433\n",
      "2401     424\n",
      "2221     396\n",
      "2122     388\n",
      "2181     297\n",
      "2211     285\n",
      "2402     285\n",
      "2312     222\n",
      "2152     209\n",
      "2121     198\n",
      "2331     154\n",
      "2301     141\n",
      "2162     141\n",
      "2302     106\n",
      "2191      86\n",
      "2231      75\n",
      "2142      75\n",
      "2182      32\n",
      "2212      23\n",
      "2232      19\n",
      "2322      15\n",
      "2171      11\n",
      "2222       7\n",
      "2192       4\n",
      "2172       3\n",
      "2332       1\n",
      "Name: ClassifB, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#### 2) Prepare the data for the classification ##############\n",
    "##############################################################\n",
    "\n",
    "#############2.1 Select level of work and classes\n",
    "#copy values in a new column 'Classif' that we will use in the rest of the script\n",
    "pd_lucas['Classif']=pd_lucas[level]\n",
    "print(pd_lucas.shape)\n",
    "\n",
    "#add the biome after the class\n",
    "pd_lucas_biome=pd_lucas[pd_lucas.Classif.isin(classes_B)]\n",
    "pd_lucas_nobiome=pd_lucas[pd_lucas.Classif.isin(classes_NB)]\n",
    "\n",
    "pd_lucas_biome['ClassifB']=pd_lucas_biome['Classif'].astype(str) + pd_lucas_biome['stratum'].astype(str)\n",
    "pd_lucas_nobiome['ClassifB']=pd_lucas_nobiome['Classif'].astype(str) + '0'\n",
    "\n",
    "pd_lucas_b=pd_lucas_biome.append(pd_lucas_nobiome)\n",
    "\n",
    "#legend level 1 - create new column and copy values\n",
    "#pd_level1['ClassL1B']=pd_level1[['LC1_COD', 'BIOME_N']].apply(lambda x: ''.join(x.map(str)), axis=1)\n",
    "#pd_level1['ClassL1B']=pd_level1['ClassL1'].astype(str) + pd_level1['BIOME_N'].astype(str)\n",
    "#print(pd_lucas_b.head())\n",
    "print(pd_lucas_b.ClassifB.value_counts())\n",
    "\n",
    "#############2.2 Prepare the dataframe with the pixels extraction\n",
    "\n",
    "lucas_polygons['Classif']=lucas_polygons.level_2\n",
    "\n",
    "#reclassify\n",
    "#lucas_polygons.Classif=lucas_polygons.Classif.replace(classes_in,\n",
    "#                                                        classes_remap)\n",
    "#print(lucas_polygons.shape)\n",
    "#print(lucas_polygons.Classif.value_counts())\n",
    "\n",
    "#select the classes of interest for Level 1\n",
    "#add the biome after the class\n",
    "\n",
    "lucas_polygons_biome=lucas_polygons[lucas_polygons.Classif.isin(classes_B)]\n",
    "lucas_polygons_nobiome=lucas_polygons[lucas_polygons.Classif.isin(classes_NB)]\n",
    "\n",
    "print(lucas_polygons_biome.shape)\n",
    "print(lucas_polygons_biome.Classif.value_counts())\n",
    "print(lucas_polygons_nobiome.shape)\n",
    "print(lucas_polygons_nobiome.Classif.value_counts())\n",
    "\n",
    "lucas_polygons_biome['ClassifB']=lucas_polygons_biome['Classif'].astype(str) + lucas_polygons_biome['stratum'].astype(str)\n",
    "lucas_polygons_nobiome['ClassifB']=lucas_polygons_nobiome['Classif'].astype(str) + '0'\n",
    "print(lucas_polygons_biome.ClassifB.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the benchmarking on the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NAME_indice   NAME_date                                       REGEX_indice  \\\n",
      "0          VV  MONTH[1-7]                                   ((?<![\\w\\d])VV_)   \n",
      "1          VH  MONTH[1-7]                                   ((?<![\\w\\d])VH_)   \n",
      "2        VHVV  MONTH[1-7]                                 ((?<![\\w\\d])VHVV_)   \n",
      "3       VV-VH  MONTH[1-7]                (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "4  VV-VH-VHVV  MONTH[1-7]  (((?<![\\w\\d])VV_)|((?<![\\w\\d])VH_)|((?<![\\w\\d]...   \n",
      "5     VV-VHVV  MONTH[1-7]              (((?<![\\w\\d])VV_)|((?<![\\w\\d])VHVV_))   \n",
      "6     VH-VHVV  MONTH[1-7]              (((?<![\\w\\d])VH_)|((?<![\\w\\d])VHVV_))   \n",
      "\n",
      "     REGEX_time     TEST  month                  name  \\\n",
      "0  (20180[1-7])  indices    NaN          VVMONTH[1-7]   \n",
      "1  (20180[1-7])  indices    NaN          VHMONTH[1-7]   \n",
      "2  (20180[1-7])  indices    NaN        VHVVMONTH[1-7]   \n",
      "3  (20180[1-7])  indices    NaN       VV-VHMONTH[1-7]   \n",
      "4  (20180[1-7])  indices    NaN  VV-VH-VHVVMONTH[1-7]   \n",
      "5  (20180[1-7])  indices    NaN     VV-VHVVMONTH[1-7]   \n",
      "6  (20180[1-7])  indices    NaN     VH-VHVVMONTH[1-7]   \n",
      "\n",
      "                                               regex  \n",
      "0                       ((?<![\\w\\d])VV_)(20180[1-7])  \n",
      "1                       ((?<![\\w\\d])VH_)(20180[1-7])  \n",
      "2                     ((?<![\\w\\d])VHVV_)(20180[1-7])  \n",
      "3    (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-7])  \n",
      "4  (((?<![\\w\\d])VV_)|((?<![\\w\\d])VH_)|((?<![\\w\\d]...  \n",
      "5  (((?<![\\w\\d])VV_)|((?<![\\w\\d])VHVV_))(20180[1-7])  \n",
      "6  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VHVV_))(20180[1-7])  \n"
     ]
    }
   ],
   "source": [
    "parameters = pd.read_csv( os.path.join(project_path,'table/RF-parameters-table-INDICE_v2.csv'))\n",
    "parameters['name']=parameters['NAME_indice']+parameters['NAME_date']\n",
    "parameters['regex']=parameters['REGEX_indice']+parameters['REGEX_time']\n",
    "print (parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Split on polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manip='INDICES-BIOME-STRATIFY-CROP_pol'\n",
    "if not os.path.exists(os.path.join('result',manip)):\n",
    "    os.mkdir(os.path.join('result',manip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option when the biomes are separated and put back together\n",
    "for i_test in range(0,len(parameters['name'])):\n",
    "    print('processing : '+manip+'  ' +parameters['name'][i_test])\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2332']\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2194']\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2234']\n",
    "    \n",
    "    #subset by biomes and create another loop for the 4 biomes\n",
    "    #execute the split/train\n",
    "    #join the results and calculate the OA\n",
    "    y_test_s_all=pd.Series([])\n",
    "    y_test_pred_s_all=pd.Series([])\n",
    "    y_test_s_all_pol=pd.Series([])\n",
    "    y_test_pred_s_all_pol=pd.Series([])\n",
    "    \n",
    "    for biome in range(1,3):\n",
    "        print(biome)\n",
    "        # 1 / create a text file for log recording\n",
    "        file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_1_Processing_Log.txt'),\"w\") \n",
    "\n",
    "        file.write('Processing summary \\n') \n",
    "        file.write(\"Date and time start: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+\"\\n\") \n",
    "        file.write(\"Classes : \"+ str(classes)+\"\\n\") \n",
    "        file.write(\"Regex : \"+ str(parameters['regex'][i_test])+\"\\n\") \n",
    "        file.write(\"Name : \"+ str(parameters['name'][i_test])+\"\\n\") \n",
    "    \n",
    "        #select biome on the polygons\n",
    "        lucas_polygons_biome_b=lucas_polygons_biome[lucas_polygons_biome.stratum.isin([biome])]\n",
    "        lucas_polygons_b=lucas_polygons_biome_b.append(lucas_polygons_nobiome)\n",
    "        #drop 2143 as there is only one\n",
    "        #lucas_polygons_b = lucas_polygons_b[lucas_polygons_b.ClassifB != 2143]\n",
    "\n",
    "        #print('dataframe complet',lucas_polygons_b.shape)\n",
    "        #variety of classes per pixels for the selected biome\n",
    "        #print('dataframe complet',pd.value_counts(lucas_polygons_b.Classif,sort=True))\n",
    "        #print('dataframe complet',lucas_polygons_b.head())\n",
    "        print(lucas_polygons_b.Classif.value_counts())\n",
    "\n",
    "        # Subset the polygons\n",
    "        X_featuresP=lucas_polygons_b.filter(items=['POINT_ID','Classif'])\n",
    "        y_classP=lucas_polygons_b['Classif']#.astype(np.float32)\n",
    "        file.write(\"Input DB polygons shape  : \"+ str(X_featuresP.shape)+\"\\n\") \n",
    "        file.write(\"Input DB polygons columns  : \"+ str(list(X_featuresP.columns))+\"\\n\") \n",
    "    \n",
    "        # 1/ Split between test and train\n",
    "        #TO BE DONE ON THE LUCAS POLYGONS\n",
    "        #https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn\n",
    "        X_trainP,X_testP,y_trainP,y_testP  = train_test_split(X_featuresP,y_classP, test_size=0.2,random_state=5,stratify=y_classP)\n",
    "        file.write(\"X_trainP.shape  : \"+ str(X_trainP.shape)+\"\\n\") \n",
    "        file.write(\"X_testP.shape  : \"+ str(X_testP.shape)+\"\\n\")\n",
    "        file.write(\"y_trainP.shape  : \"+ str(y_trainP.shape)+\"\\n\")\n",
    "        file.write(\"y_testP.shape  : \"+ str(y_testP.shape)+\"\\n\")\n",
    "\n",
    "        # 2/select the pixels from the polygons\n",
    "        #and Subset the DB with regex\n",
    "        indexPOINItrain=pd_lucas_b['POINT_ID'].isin(X_trainP['POINT_ID'])\n",
    "        indexPOINItest=pd_lucas_b['POINT_ID'].isin(X_testP['POINT_ID'])\n",
    "        \n",
    "        X_train=pd_lucas_b[indexPOINItrain].filter(regex=parameters['regex'][i_test])\n",
    "        y_train=pd_lucas_b[indexPOINItrain]['Classif']\n",
    "        X_test=pd_lucas_b[indexPOINItest].filter(regex=parameters['regex'][i_test])\n",
    "        y_test=pd_lucas_b[indexPOINItest]['Classif']\n",
    "        \n",
    "        #write\n",
    "        file.write(\"Input DB X_train pixels shape  : \"+ str(X_train.shape)+\"\\n\") \n",
    "        file.write(\"Input DB X_train pixels columns  : \"+ str(list(X_train.columns))+\"\\n\") \n",
    "        \n",
    "        #keep all info to aggregate prediction per polygons\n",
    "        y_train_pol=pd_lucas_b[indexPOINItrain]\n",
    "        y_test_pol=pd_lucas_b[indexPOINItest]\n",
    "\n",
    "       \n",
    "        # 4/ Save the class distribution for training and testing as CSV\n",
    "        #x = pd.DataFrame(y_train.value_counts().rename_axis('class').reset_index(name='counts'))\n",
    "        x = pd.DataFrame({\"count_pol\": y_train_pol.groupby('POINT_ID').apply(max)['Classif'].value_counts(), \"count_pixel\": y_train.value_counts()}).rename_axis('class')\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_2_Training_class_count_polygons.csv'))\n",
    "        #x = pd.DataFrame(y_test.value_counts().rename_axis('class').reset_index(name='counts'))\n",
    "        x = pd.DataFrame({\"count_pol\": y_test_pol.groupby('POINT_ID').apply(max)['Classif'].value_counts(), \"count_pixel\": y_test.value_counts()}).rename_axis('class')\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_3_Testing_class_count_polygons.csv'))\n",
    "\n",
    "\n",
    "        # 5/ Fit the RANDOM PARAMETERS T\n",
    "        t = time.time()    \n",
    "        clf = RandomForestClassifier(bootstrap=0, criterion='gini', max_depth=None, max_features='auto', \n",
    "                                     min_samples_leaf=12, min_samples_split=3, n_estimators=800, n_jobs=40)\n",
    "                                                                                                                                                                                    \n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        training_time=time.time() - t\n",
    "        file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_1_Processing_Log.txt'),\"a\") \n",
    "        file.write(\"Elapsed time for training  : \"+ str(round(training_time))+\" sec \\n\")\n",
    "        #file.write(\"Model  : \" +str(clf)+\"\\n\")\n",
    "        file.close()\n",
    "\n",
    "        # 6/ Feature importances as  CSV\n",
    "        x = list(zip(clf.feature_importances_,X_train.columns))\n",
    "        x = pd.DataFrame(x,columns=[\"Importance\",\"Feature_Name\"])\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_2_Feature_importance.csv') )\n",
    "        \n",
    "        #append the test value in a file for the 4 biomes\n",
    "        # 7/ OA -evaluate accuracy with the test dataset for the unique rf model\n",
    "        #reclassify the classes by biomes to the simple classes \n",
    "        #transform to series to use replace\n",
    "\n",
    "        #Prediction\n",
    "        y_test_pred=clf.predict(X_test)      \n",
    "        y_test_s=pd.Series(y_test, dtype='float')\n",
    "        #y_test_s=y_test_s.replace(classes_classif,classes_classif_simplify)\n",
    "        \n",
    "        y_test_pred_s=pd.Series(y_test_pred, dtype='float')\n",
    "        #y_test_pred_s=y_test_pred_s.replace(classes_classif,classes_classif_simplify)\n",
    "                \n",
    "        #to calculate accuracy, go back to array    \n",
    "        accuracy = 100.0*(y_test_s.array == y_test_pred_s.array).sum()/y_test_s.shape[0]\n",
    "        print('Accuracy is :' + str(round(accuracy,2)))\n",
    "    \n",
    "        #del(file)\n",
    "        file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_1_1_Accuracy.txt'),\"w\") \n",
    "        #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "        file1.write(str(accuracy)+\"\\n\") \n",
    "        file1.close()\n",
    "        \n",
    "        # 8/ Classification report\n",
    "        report = classification_report(y_test_s, y_test_pred_s, output_dict=True)\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_3_classification_report.csv') )\n",
    "        \n",
    "        # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "        confusion_mat=confusion_matrix(y_test_s,y_test_pred_s,labels=classes)\n",
    "        confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "        confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_4_confusion_matrix_class.csv'))\n",
    "        \n",
    "        #accuracy mode polygon\n",
    "        #apply a majority rule (mode)\n",
    "        #group it with y_test\n",
    "        y_test_results=pd.DataFrame({'POINT_ID':y_test_pol['POINT_ID'],'ref':y_test,'predict':y_test_pred})\n",
    "        y_test_results=y_test_results.groupby(['POINT_ID'])['predict','ref'].agg(lambda x: x.mode()[0])\n",
    "        \n",
    "        #to calculate accuracy, go back to array    \n",
    "        accuracy_pol = 100.0*(y_test_results['ref'].array == y_test_results['predict'].array).sum()/y_test_results.shape[0]\n",
    "        print('Accuracy is :' + str(round(accuracy_pol,2)))\n",
    "        \n",
    "        #del(file)\n",
    "        file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_1_1_Accuracy_pol.txt'),\"w\") \n",
    "        #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "        file1.write(str(accuracy_pol)+\"\\n\") \n",
    "        file1.close()\n",
    "        # 8/ Classification report\n",
    "        report = classification_report(y_test_results['ref'],y_test_results['predict'], output_dict=True)\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_3_classification_report_pol.csv') )\n",
    "        \n",
    "        # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "        confusion_mat=confusion_matrix(y_test_results['ref'],y_test_results['predict'],labels=classes)\n",
    "        confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "        confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_4_confusion_matrix_class_pol.csv'))\n",
    "        \n",
    "        y_test_s_all=y_test_s_all.append(y_test_s)      \n",
    "        #print(y_test_all)        \n",
    "        y_test_pred_s_all=y_test_pred_s_all.append(y_test_pred_s)\n",
    "        \n",
    "        #pol\n",
    "        y_test_s_all_pol=y_test_s_all_pol.append(y_test_results['ref'])      \n",
    "        #print(y_test_all)        \n",
    "        y_test_pred_s_all_pol=y_test_pred_s_all_pol.append(y_test_results['predict'])\n",
    "        \n",
    "    #to calculate accuracy, go back to array    \n",
    "    accuracy = 100.0*(y_test_s_all.array == y_test_pred_s_all.array).sum()/y_test_s_all.shape[0]\n",
    "    print('Accuracy is :' + str(round(accuracy,2)))\n",
    "   \n",
    "    #del(file)\n",
    "    file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Accuracy.txt'),\"w\") \n",
    "    #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "    file1.write(str(accuracy)+\"\\n\") \n",
    "    file1.close()\n",
    "    \n",
    "    # 8/ Classification report\n",
    "    report = classification_report(y_test_s_all, y_test_pred_s_all, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_3_classification_report.csv') )\n",
    "\n",
    "    # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "    confusion_mat=confusion_matrix(y_test_s_all,y_test_pred_s_all,labels=classes)\n",
    "    confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "    confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_4_confusion_matrix_class.csv'))\n",
    "    #confusion_mat_class=pd.DataFrame(confusion_mat,index= list(labels_s['class']),columns=list(labels_s['class']))\n",
    "    #confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_remap_4_confusion_matrix_class.csv') )\n",
    "    #confusion_mat_label=pd.DataFrame(confusion_mat,index= list(labels_s['label']),columns=list(labels_s['label']))\n",
    "    #confusion_mat_label.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_remap_4_confusion_matrix_label.csv') )\n",
    "\n",
    "    #to calculate accuracy, go back to array    \n",
    "    accuracy_pol = 100.0*(y_test_s_all_pol.array == y_test_pred_s_all_pol.array).sum()/y_test_s_all_pol.shape[0]\n",
    "    print('Accuracy is :' + str(round(accuracy_pol,2)))\n",
    "    \n",
    "    #del(file)\n",
    "    file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Accuracy_pol.txt'),\"w\") \n",
    "    #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "    file1.write(str(accuracy_pol)+\"\\n\") \n",
    "    file1.close()\n",
    "    \n",
    "    # 8/ Classification report\n",
    "    report = classification_report(y_test_s_all_pol, y_test_pred_s_all_pol, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_3_classification_report_pol.csv') )\n",
    "\n",
    "    # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "    confusion_mat=confusion_matrix(y_test_s_all_pol,y_test_pred_s_all_pol,labels=classes)\n",
    "    confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "    confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_4_confusion_matrix_class_pol.csv'))\n",
    "    \n",
    "    file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Processing_Log.txt'),\"a\") \n",
    "    file.write(\"Date and time end: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+\"\\n\") \n",
    "    file.close()\n",
    "    \n",
    "    del(df,clf,confusion_mat)#confusion_mat_label,confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Split on pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manip='INDICES-BIOME-STRATIFY-CROP_pix'\n",
    "if not os.path.exists(os.path.join('result',manip)):\n",
    "    os.mkdir(os.path.join('result',manip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option when the biomes are separated and put back together\n",
    "for i_test in range(0,len(parameters['name'])):\n",
    "    print('processing : '+manip+'  ' +parameters['name'][i_test])\n",
    "    lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2332']\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2194']\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2234']\n",
    "    \n",
    "    #subset by biomes and create another loop for the 4 biomes\n",
    "    #execute the split/train\n",
    "    #join the results and calculate the OA\n",
    "    y_test_s_all=pd.Series([])\n",
    "    y_test_pred_s_all=pd.Series([])\n",
    "\n",
    "    \n",
    "    for biome in range(1,3):\n",
    "        print(biome)\n",
    "        # 1 / create a text file for log recording\n",
    "        file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_1_Processing_Log.txt'),\"w\") \n",
    "\n",
    "        file.write('Processing summary \\n') \n",
    "        file.write(\"Date and time start: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+\"\\n\") \n",
    "        file.write(\"Classes : \"+ str(classes)+\"\\n\") \n",
    "        file.write(\"Regex : \"+ str(parameters['regex'][i_test])+\"\\n\") \n",
    "        file.write(\"Name : \"+ str(parameters['name'][i_test])+\"\\n\") \n",
    "    \n",
    "        #select biome on the polygons\n",
    "        lucas_polygons_biome_b=lucas_polygons_biome[lucas_polygons_biome.stratum.isin([biome])]\n",
    "        lucas_polygons_b=lucas_polygons_biome_b.append(lucas_polygons_nobiome)\n",
    "        #drop 2143 as there is only one\n",
    "        #lucas_polygons_b = lucas_polygons_b[lucas_polygons_b.ClassifB != 2143]\n",
    "\n",
    "        #print('dataframe complet',lucas_polygons_b.shape)\n",
    "        #variety of classes per pixels for the selected biome\n",
    "        #print('dataframe complet',pd.value_counts(lucas_polygons_b.Classif,sort=True))\n",
    "        #print('dataframe complet',lucas_polygons_b.head())\n",
    "        print(lucas_polygons_b.Classif.value_counts())\n",
    "\n",
    "        # Subset the polygons\n",
    "        X_features=pd_lucas_b.filter(regex=parameters['regex'][i_test])\n",
    "        y_class=pd_lucas_b['Classif']#.astype(np.float32)\n",
    "        file.write(\"Input DB pixel shape  : \"+ str(X_features.shape)+\"\\n\") \n",
    "        file.write(\"Input DB pixel columns  : \"+ str(list(X_features.columns))+\"\\n\") \n",
    "    \n",
    "        # 1/ Split between test and train\n",
    "        #TO BE DONE ON THE LUCAS POLYGONS\n",
    "        #https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn\n",
    "        X_train,X_test,y_train,y_test  = train_test_split(X_features,y_class, test_size=0.2,random_state=5,stratify=y_class)\n",
    "        \n",
    "        file.write(\"X_train.shape  : \"+ str(X_train.shape)+\"\\n\") \n",
    "        file.write(\"X_test.shape  : \"+ str(X_test.shape)+\"\\n\")\n",
    "        file.write(\"y_train.shape  : \"+ str(y_train.shape)+\"\\n\")\n",
    "        file.write(\"y_test.shape  : \"+ str(y_test.shape)+\"\\n\")\n",
    "\n",
    "        # 2/select the pixels from the polygons\n",
    "        #and Subset the DB with regex\n",
    "             \n",
    "        #write\n",
    "        file.write(\"Input DB X_train pixels shape  : \"+ str(X_train.shape)+\"\\n\") \n",
    "        file.write(\"Input DB X_train pixels columns  : \"+ str(list(X_train.columns))+\"\\n\") \n",
    "        \n",
    "      \n",
    "        # 4/ Save the class distribution for training and testing as CSV\n",
    "        #x = pd.DataFrame(y_train.value_counts().rename_axis('class').reset_index(name='counts'))\n",
    "        x = pd.DataFrame({\"count_pixel\": y_train.value_counts()}).rename_axis('class')\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_2_Training_class_count_pixels.csv'))\n",
    "        #x = pd.DataFrame(y_test.value_counts().rename_axis('class').reset_index(name='counts'))\n",
    "        x = pd.DataFrame({\"count_pixel\": y_test.value_counts()}).rename_axis('class')\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_3_Testing_class_count_pixels.csv'))\n",
    "\n",
    "\n",
    "        # 5/ Fit the RANDOM PARAMETERS T\n",
    "        t = time.time()    \n",
    "        clf = RandomForestClassifier(bootstrap=0, criterion='gini', max_depth=None, max_features='auto', \n",
    "                                     min_samples_leaf=12, min_samples_split=3, n_estimators=800, n_jobs=40)\n",
    "                                                                                                                                                                                    \n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        training_time=time.time() - t\n",
    "        file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_1_Processing_Log.txt'),\"a\") \n",
    "        file.write(\"Elapsed time for training  : \"+ str(round(training_time))+\" sec \\n\")\n",
    "        #file.write(\"Model  : \" +str(clf)+\"\\n\")\n",
    "        file.close()\n",
    "\n",
    "        # 6/ Feature importances as  CSV\n",
    "        x = list(zip(clf.feature_importances_,X_train.columns))\n",
    "        x = pd.DataFrame(x,columns=[\"Importance\",\"Feature_Name\"])\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_2_Feature_importance.csv') )\n",
    "        \n",
    "        #append the test value in a file for the 4 biomes\n",
    "        # 7/ OA -evaluate accuracy with the test dataset for the unique rf model\n",
    "        #reclassify the classes by biomes to the simple classes \n",
    "        #transform to series to use replace\n",
    "\n",
    "        #Prediction\n",
    "        y_test_pred=clf.predict(X_test)      \n",
    "        y_test_s=pd.Series(y_test, dtype='float')\n",
    "        #y_test_s=y_test_s.replace(classes_classif,classes_classif_simplify)\n",
    "        \n",
    "        y_test_pred_s=pd.Series(y_test_pred, dtype='float')\n",
    "        #y_test_pred_s=y_test_pred_s.replace(classes_classif,classes_classif_simplify)\n",
    "                \n",
    "        #to calculate accuracy, go back to array    \n",
    "        accuracy = 100.0*(y_test_s.array == y_test_pred_s.array).sum()/y_test_s.shape[0]\n",
    "        print('Accuracy is :' + str(round(accuracy,2)))\n",
    "    \n",
    "        #del(file)\n",
    "        file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_1_1_Accuracy.txt'),\"w\") \n",
    "        #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "        file1.write(str(accuracy)+\"\\n\") \n",
    "        file1.close()\n",
    "        \n",
    "        # 8/ Classification report\n",
    "        report = classification_report(y_test_s, y_test_pred_s, output_dict=True)\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_3_classification_report.csv') )\n",
    "        \n",
    "        # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "        confusion_mat=confusion_matrix(y_test_s,y_test_pred_s,labels=classes)\n",
    "        confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "        confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_4_confusion_matrix_class.csv'))\n",
    "        \n",
    "        \n",
    "        y_test_s_all=y_test_s_all.append(y_test_s)      \n",
    "        #print(y_test_all)        \n",
    "        y_test_pred_s_all=y_test_pred_s_all.append(y_test_pred_s)\n",
    "    \n",
    "        \n",
    "    #to calculate accuracy, go back to array    \n",
    "    accuracy = 100.0*(y_test_s_all.array == y_test_pred_s_all.array).sum()/y_test_s_all.shape[0]\n",
    "    print('Accuracy is :' + str(round(accuracy,2)))\n",
    "   \n",
    "    #del(file)\n",
    "    file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Accuracy.txt'),\"w\") \n",
    "    #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "    file1.write(str(accuracy)+\"\\n\") \n",
    "    file1.close()\n",
    "    \n",
    "    # 8/ Classification report\n",
    "    report = classification_report(y_test_s_all, y_test_pred_s_all, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_3_classification_report.csv') )\n",
    "\n",
    "    # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "    confusion_mat=confusion_matrix(y_test_s_all,y_test_pred_s_all,labels=classes)\n",
    "    confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "    confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_4_confusion_matrix_class.csv'))\n",
    "    #confusion_mat_class=pd.DataFrame(confusion_mat,index= list(labels_s['class']),columns=list(labels_s['class']))\n",
    "    #confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_remap_4_confusion_matrix_class.csv') )\n",
    "    #confusion_mat_label=pd.DataFrame(confusion_mat,index= list(labels_s['label']),columns=list(labels_s['label']))\n",
    "    #confusion_mat_label.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_remap_4_confusion_matrix_label.csv') )\n",
    "\n",
    "    file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Processing_Log.txt'),\"a\") \n",
    "    file.write(\"Date and time end: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+\"\\n\") \n",
    "    file.close()\n",
    "    \n",
    "    del(df,clf,confusion_mat)#confusion_mat_label,confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
