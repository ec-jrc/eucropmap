{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EU crop map - Benchmarking on the time period for the mask classes\n",
    "## 1) Split by polygones - accuracy per pixels and per polygone\n",
    "## 2) Split by pixels - accuracy per pixels\n",
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JEODPP\n",
    "data_path='/eos/jeodpp/data/projects/REFOCUS/data/S1_GS/all-10days/Map_v7/'\n",
    "project_path='/eos/jeodpp/data/projects/REFOCUS/classification/'\n",
    "path_pol = '/eos/jeodpp/data/projects/REFOCUS/data/polygons/v7'\n",
    "results='/eos/jeodpp/data/projects/REFOCUS/classification/'\n",
    "\n",
    "local='/eos/jeodpp/home/users/verheas/data/LUCAS/v7/'\n",
    "\n",
    "#working directory\n",
    "pwd = project_path\n",
    "\n",
    "# !pip install matplotlib --user\n",
    "# !pip install sklearn --user\n",
    "#import \n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd_lucas (2956889, 116)\n",
      "300    1216530\n",
      "200    1000318\n",
      "500     732964\n",
      "600       3856\n",
      "100       3221\n",
      "Name: level_1, dtype: int64\n",
      "300    1216530\n",
      "500     732964\n",
      "211     290116\n",
      "213     142886\n",
      "216     125644\n",
      "232      64452\n",
      "290      63609\n",
      "250      59053\n",
      "214      35215\n",
      "231      34577\n",
      "240      34067\n",
      "215      34005\n",
      "212      28825\n",
      "222      23174\n",
      "218      19274\n",
      "221      15815\n",
      "230      12000\n",
      "233       7094\n",
      "223       4920\n",
      "219       4724\n",
      "600       3856\n",
      "100       3221\n",
      "217        868\n",
      "Name: level_2, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (27,30,33,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>POINT_ID</th>\n",
       "      <th>NUTS0</th>\n",
       "      <th>NUTS1</th>\n",
       "      <th>NUTS2</th>\n",
       "      <th>NUTS3</th>\n",
       "      <th>TH_LAT</th>\n",
       "      <th>TH_LONG</th>\n",
       "      <th>OFFICE_PI</th>\n",
       "      <th>EX_ANTE</th>\n",
       "      <th>...</th>\n",
       "      <th>LU2_LABEL</th>\n",
       "      <th>LU1_TYPE_LABEL</th>\n",
       "      <th>LU2_TYPE_LABEL</th>\n",
       "      <th>CPRN_LC_LABEL</th>\n",
       "      <th>CPRN_LC_SAME_LC1</th>\n",
       "      <th>LUCAS_CORE_INTERSECT</th>\n",
       "      <th>COPERNICUS_CLEANED</th>\n",
       "      <th>stratum</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34562080</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES2</td>\n",
       "      <td>ES24</td>\n",
       "      <td>ES243</td>\n",
       "      <td>41.288386</td>\n",
       "      <td>-0.319428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other bare soil</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>290</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31243520</td>\n",
       "      <td>IE</td>\n",
       "      <td>IE0</td>\n",
       "      <td>IE04</td>\n",
       "      <td>IE042</td>\n",
       "      <td>53.422977</td>\n",
       "      <td>-8.226052</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spontaneously re-vegetated surfaces</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>33661774</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES5</td>\n",
       "      <td>ES52</td>\n",
       "      <td>ES521</td>\n",
       "      <td>38.434388</td>\n",
       "      <td>-0.905705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Permanent crops: fruit trees</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>28922250</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES1</td>\n",
       "      <td>ES11</td>\n",
       "      <td>ES113</td>\n",
       "      <td>41.867145</td>\n",
       "      <td>-7.320304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shrubland with sparse tree cover</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>35082906</td>\n",
       "      <td>FR</td>\n",
       "      <td>FRD</td>\n",
       "      <td>FRD1</td>\n",
       "      <td>FRD12</td>\n",
       "      <td>48.715190</td>\n",
       "      <td>-1.092190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Grassland without tree/shrub cover</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  POINT_ID NUTS0 NUTS1 NUTS2  NUTS3     TH_LAT   TH_LONG  \\\n",
       "0           0  34562080    ES   ES2  ES24  ES243  41.288386 -0.319428   \n",
       "1           1  31243520    IE   IE0  IE04  IE042  53.422977 -8.226052   \n",
       "2           2  33661774    ES   ES5  ES52  ES521  38.434388 -0.905705   \n",
       "3           3  28922250    ES   ES1  ES11  ES113  41.867145 -7.320304   \n",
       "4           4  35082906    FR   FRD  FRD1  FRD12  48.715190 -1.092190   \n",
       "\n",
       "   OFFICE_PI  EX_ANTE  ...     LU2_LABEL  LU1_TYPE_LABEL  LU2_TYPE_LABEL  \\\n",
       "0          0        0  ...  Not relevant             NaN             NaN   \n",
       "1          0        0  ...  Not relevant             NaN             NaN   \n",
       "2          0        0  ...  Not relevant             NaN             NaN   \n",
       "3          0        0  ...  Not relevant             NaN             NaN   \n",
       "4          0        0  ...  Not relevant             NaN             NaN   \n",
       "\n",
       "                         CPRN_LC_LABEL  CPRN_LC_SAME_LC1  \\\n",
       "0                      Other bare soil              True   \n",
       "1  Spontaneously re-vegetated surfaces              True   \n",
       "2         Permanent crops: fruit trees              True   \n",
       "3     Shrubland with sparse tree cover              True   \n",
       "4   Grassland without tree/shrub cover              True   \n",
       "\n",
       "   LUCAS_CORE_INTERSECT  COPERNICUS_CLEANED  stratum  level_2  level_1  \n",
       "0                  True                True        2      290      200  \n",
       "1                  True                True        1      500      500  \n",
       "2                  True                True        2      300      300  \n",
       "3                  True                True        2      300      300  \n",
       "4                  True                True        1      500      500  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the data\n",
    "#1) load the S1 10 days extracted values in GEE for all polygons\n",
    "\n",
    "pd_lucas= pd.read_csv(os.path.join(data_path,'S1_point_allV7_10days_10m_1Jan-31Dec_EU_ratio-db.csv'),dtype={'level_1':int,'level_2':int})\n",
    "print('pd_lucas',pd_lucas.shape)\n",
    "\n",
    "#concatenate all the data in one dataframe\n",
    "#group cropland, grassland and bareland \n",
    "#number of pixels per class\n",
    "print(pd_lucas.level_1.value_counts())\n",
    "print(pd_lucas.level_2.value_counts())\n",
    "pd_lucas.head()\n",
    "\n",
    "#number of pixels per class\n",
    "#pd_lucas.LC1_COD.value_counts()\n",
    "#pd_lucas.head()\n",
    "pd_lucas.columns\n",
    "\n",
    "##############1.2 Load the shapefile with the polygons - useful to split the polygons in training and test dataset for the accuracy ######################\n",
    "# load csv with of the polygons\n",
    "#2)load csv with the polygons for the split test/validation\n",
    "lucas_polygons = pd.read_csv(os.path.join(path_pol,'LUCAS_2018_Copernicus_attributes_cropmap_level1-2.csv'))\n",
    "lucas_polygons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_B [200, 300, 500]\n",
      "classes_NB [100, 600]\n",
      "level level_1\n",
      "level [100, 200, 300, 500, 600]\n",
      "classes_classif [1000, 2001, 2002, 2003, 2004, 3001, 3002, 3003, 3004, 5001, 5002, 5003, 5004, 6000]\n",
      "classes_classif_simplify [100, 200, 200, 200, 200, 300, 300, 300, 300, 500, 500, 500, 500, 600]\n",
      "[100, 200, 300, 500, 600]\n",
      "   class                          label\n",
      "0    100                Artificial land\n",
      "1    200  Cropland without trees/shrubs\n",
      "2    300                       Woodland\n",
      "4    600                      Bare land\n",
      "   class                          label\n",
      "0    100                Artificial land\n",
      "1    200  Cropland without trees/shrubs\n",
      "2    300                       Woodland\n",
      "4    600                      Bare land\n"
     ]
    }
   ],
   "source": [
    "##################################Parameters##################################################\n",
    "#classes - stored in a table 'legend-lucas-all'\n",
    "table_class=pd.read_csv(os.path.join(project_path,'table/legend-lucas-all-v7.csv'),dtype=pd.Int64Dtype())\n",
    "\n",
    "classes_L1=list(table_class['classes_L1'].dropna()) \n",
    "classes_L2=list(table_class['classes_L2'].dropna())\n",
    "\n",
    "#level\n",
    "level_1='level_1'\n",
    "level_2='level_2'\n",
    "\n",
    "##################################Parameters##################################################\n",
    "#classes - stored in a table 'legend-lucas-all'\n",
    "table_class=pd.read_csv(os.path.join(project_path,'table/legend-lucas-all-v2.csv'),dtype=pd.Int64Dtype())\n",
    "\n",
    "classes_L1=list(table_class['classes_L1'].dropna())\n",
    "classes_L2=list(table_class['classes_L2'].dropna())\n",
    "\n",
    "#remap classes and selection of classes to map Level 1\n",
    "classes_in_L1 =  list(table_class['classes_all'].dropna())\n",
    "\n",
    "classes_in_L2 = list(table_class['classes_all'].dropna()),\n",
    "\n",
    "#classes affected by biome selection\n",
    "classes_L1_B= list(table_class['classes_L1_B'].dropna())\n",
    "classes_L2_B= list(table_class['classes_L2'].dropna())\n",
    "\n",
    "#Classes non affected by biome selection\n",
    "#Classes from (A) Artificial, (F) Bare lands and (H) Wetlands can be considered in each models - no biome dependent\n",
    "classes_L1_NB=list(table_class['classes_L1_NB'].dropna())\n",
    "classes_L2_NB=[]\n",
    "#summary of the classes used in the classification\n",
    "classes_classif_L1= list(table_class['L1_BIOME'].dropna())\n",
    "classes_classif_L1_simplify=list(table_class['L1_B_harmon'].dropna())\n",
    "\n",
    "classes_classif_L2=list(table_class['L2_BIOME'].dropna())\n",
    "classes_classif_L2_simplify=list(table_class['L2_B_harmon'].dropna())\n",
    "\n",
    "###################################Choose parameters for this run #############################################\n",
    "#classes for the classification and biome/no biome differentiation if needed\n",
    "classes_B=classes_L1_B\n",
    "print ('classes_B',classes_B)\n",
    "\n",
    "classes_NB=classes_L1_NB\n",
    "print ('classes_NB',classes_NB)\n",
    "\n",
    "#level\n",
    "level=level_1\n",
    "print('level',level)\n",
    "#crop - level 2, from the table we load only the crop type classes\n",
    "classes=classes_L1\n",
    "print('level',classes)\n",
    "\n",
    "#Split for the train/test dataset - we run it with all the polygons\n",
    "#split_test = 0\n",
    "\n",
    "#summary of the classses used for the classification\n",
    "classes_classif=classes_classif_L1\n",
    "print ('classes_classif',classes_classif)\n",
    "classes_classif_simplify=classes_classif_L1_simplify\n",
    "print ('classes_classif_simplify',classes_classif_simplify)\n",
    "\n",
    "###################################Labels of the classes #############################################\n",
    "labels_csv = pd.read_csv(os.path.join(project_path,'table/legend-lucas2.csv'))\n",
    "labels=labels_csv[labels_csv['class'].isin(classes)] # select only the used labels\n",
    "labels_s=labels_csv[labels_csv['class'].isin(classes)] # select only the used labels\n",
    "print(classes)\n",
    "print(labels)\n",
    "print(labels_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2956889, 117)\n",
      "(57897, 114)\n",
      "300    22396\n",
      "200    18376\n",
      "500    17125\n",
      "Name: Classif, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#### 2) Prepare the data for the classification ##############\n",
    "##############################################################\n",
    "\n",
    "#############2.1 Select level of work and classes\n",
    "#copy values in a new column 'Classif' that we will use in the rest of the script\n",
    "pd_lucas['Classif']=pd_lucas[level]\n",
    "print(pd_lucas.shape)\n",
    "\n",
    "#add the biome after the class\n",
    "pd_lucas_biome=pd_lucas[pd_lucas.Classif.isin(classes_B)]\n",
    "#pd_lucas_nobiome=#pd_lucas[pd_lucas.Classif.isin(classes_NB)]\n",
    "\n",
    "#for this test\n",
    "#pd_lucas_biome['ClassifB']=pd_lucas_biome['Classif'].astype(str) + pd_lucas_biome['stratum'].astype(str)\n",
    "#pd_lucas_nobiome['ClassifB']=pd_lucas_nobiome['Classif'].astype(str) + pd_lucas_biome['stratum'].astype(str)\n",
    "\n",
    "pd_lucas_b=pd_lucas_biome#.append(pd_lucas_nobiome)\n",
    "#legend level 1 - create new column and copy values\n",
    "#pd_level1['ClassL1B']=pd_level1[['LC1_COD', 'BIOME_N']].apply(lambda x: ''.join(x.map(str)), axis=1)\n",
    "#pd_level1['ClassL1B']=pd_level1['ClassL1'].astype(str) + pd_level1['BIOME_N'].astype(str)\n",
    "#print(pd_lucas_b.head())\n",
    "#print(pd_lucas_b.ClassifB.value_counts())\n",
    "\n",
    "#2) Prepare the dataframe with the pixels extraction\n",
    "lucas_polygons['Classif']=lucas_polygons.level_1\n",
    "\n",
    "#reclassify\n",
    "#lucas_polygons.Classif=lucas_polygons.Classif.replace(classes_in,\n",
    "#                                                        classes_remap)\n",
    "#print(lucas_polygons.shape)\n",
    "#print(lucas_polygons.Classif.value_counts())\n",
    "\n",
    "#select the classes of interest for Level 1\n",
    "#add the biome after the class\n",
    "\n",
    "lucas_polygons_biome=lucas_polygons[lucas_polygons.Classif.isin(classes_B)]\n",
    "#lucas_polygons_nobiome=lucas_polygons[lucas_polygons.Classif.isin(classes_NB)]\n",
    "\n",
    "print(lucas_polygons_biome.shape)\n",
    "print(lucas_polygons_biome.Classif.value_counts())\n",
    "#print(lucas_polygons_nobiome.shape)\n",
    "#print(lucas_polygons_nobiome.Classif.value_counts())\n",
    "\n",
    "#lucas_polygons_biome['ClassifB']=lucas_polygons_biome['Classif'].astype(str) + lucas_polygons_biome['stratum'].astype(str)\n",
    "#lucas_polygons_nobiome['ClassifB']=lucas_polygons_nobiome['Classif'].astype(str) + '0'\n",
    "#print(lucas_polygons_biome.ClassifB.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the benchmarking on the time period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NAME_indice    NAME_date                         REGEX_indice  \\\n",
      "0        VV-VH   MONTH[1-1]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "1        VV-VH   MONTH[1-2]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "2        VV-VH   MONTH[1-3]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "3        VV-VH   MONTH[1-4]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "4        VV-VH   MONTH[1-5]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "5        VV-VH   MONTH[1-6]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "6        VV-VH   MONTH[1-7]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "7        VV-VH   MONTH[1-8]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "8        VV-VH   MONTH[1-9]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "9        VV-VH  MONTH[1-10]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "10       VV-VH  MONTH[1-11]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "11       VV-VH  MONTH[1-12]  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))   \n",
      "\n",
      "                 REGEX_time   TEST  month              name  \\\n",
      "0              (20180[1-1])  dates      1   VV-VHMONTH[1-1]   \n",
      "1              (20180[1-2])  dates      2   VV-VHMONTH[1-2]   \n",
      "2              (20180[1-3])  dates      3   VV-VHMONTH[1-3]   \n",
      "3              (20180[1-4])  dates      4   VV-VHMONTH[1-4]   \n",
      "4              (20180[1-5])  dates      5   VV-VHMONTH[1-5]   \n",
      "5              (20180[1-6])  dates      6   VV-VHMONTH[1-6]   \n",
      "6              (20180[1-7])  dates      7   VV-VHMONTH[1-7]   \n",
      "7              (20180[1-8])  dates      8   VV-VHMONTH[1-8]   \n",
      "8              (20180[1-9])  dates      9   VV-VHMONTH[1-9]   \n",
      "9     (20180[1-9]|20181[0])  dates     10  VV-VHMONTH[1-10]   \n",
      "10  (20180[1-9]|20181[0-1])  dates     11  VV-VHMONTH[1-11]   \n",
      "11  (20180[1-9]|20181[0-2])  dates     12  VV-VHMONTH[1-12]   \n",
      "\n",
      "                                                regex  \n",
      "0     (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-1])  \n",
      "1     (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-2])  \n",
      "2     (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-3])  \n",
      "3     (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-4])  \n",
      "4     (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-5])  \n",
      "5     (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-6])  \n",
      "6     (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-7])  \n",
      "7     (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-8])  \n",
      "8     (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-9])  \n",
      "9   (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-9]...  \n",
      "10  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-9]...  \n",
      "11  (((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-9]...  \n"
     ]
    }
   ],
   "source": [
    "parameters = pd.read_csv(os.path.join(project_path,'table/RF-parameters-table-DATE-VV-VH.csv'))\n",
    "parameters['name']=parameters['NAME_indice']+parameters['NAME_date']\n",
    "parameters['regex']=parameters['REGEX_indice']+parameters['REGEX_time']\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Split on polygons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "manip='DATE-BIOME-STRATIFY-LEVEL1_pol'\n",
    "if not os.path.exists(os.path.join('result',manip)):\n",
    "    os.mkdir(os.path.join('result',manip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing : DATE-BIOME-STRATIFY-LEVEL1_pol  VV-VHMONTH[1-1]\n",
      "1\n",
      "300    15526\n",
      "200    14142\n",
      "500    14034\n",
      "Name: Classif, dtype: int64\n",
      "Accuracy is :76.51\n",
      "Accuracy is :77.73\n",
      "2\n",
      "300    6870\n",
      "200    4234\n",
      "500    3091\n",
      "Name: Classif, dtype: int64\n",
      "Accuracy is :70.79\n",
      "Accuracy is :69.81\n",
      "Accuracy is :75.39\n",
      "Accuracy is :75.79\n",
      "processing : DATE-BIOME-STRATIFY-LEVEL1_pol  VV-VHMONTH[1-2]\n",
      "1\n",
      "300    15526\n",
      "200    14142\n",
      "500    14034\n",
      "Name: Classif, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Option when the biomes are separated and put back together\n",
    "for i_test in range(0,len(parameters['name'])):\n",
    "    print('processing : '+manip+'  ' +parameters['name'][i_test])\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2332']\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2194']\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2234']\n",
    "    \n",
    "    #subset by biomes and create another loop for the 4 biomes\n",
    "    #execute the split/train\n",
    "    #join the results and calculate the OA\n",
    "    y_test_s_all=pd.Series([])\n",
    "    y_test_pred_s_all=pd.Series([])\n",
    "    y_test_s_all_pol=pd.Series([])\n",
    "    y_test_pred_s_all_pol=pd.Series([])\n",
    "    \n",
    "    for biome in range(1,3):\n",
    "        print(biome)\n",
    "        # 1 / create a text file for log recording\n",
    "        file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_1_Processing_Log.txt'),\"w\") \n",
    "\n",
    "        file.write('Processing summary \\n') \n",
    "        file.write(\"Date and time start: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+\"\\n\") \n",
    "        file.write(\"Classes : \"+ str(classes)+\"\\n\") \n",
    "        file.write(\"Regex : \"+ str(parameters['regex'][i_test])+\"\\n\") \n",
    "        file.write(\"Name : \"+ str(parameters['name'][i_test])+\"\\n\") \n",
    "    \n",
    "        #select biome on the polygons\n",
    "        lucas_polygons_biome_b=lucas_polygons_biome[lucas_polygons_biome.stratum.isin([biome])]\n",
    "        lucas_polygons_b=lucas_polygons_biome_b#.append(lucas_polygons_nobiome)\n",
    "        #drop 2143 as there is only one\n",
    "        #lucas_polygons_b = lucas_polygons_b[lucas_polygons_b.ClassifB != 2143]\n",
    "\n",
    "        #print('dataframe complet',lucas_polygons_b.shape)\n",
    "        #variety of classes per pixels for the selected biome\n",
    "        #print('dataframe complet',pd.value_counts(lucas_polygons_b.Classif,sort=True))\n",
    "        #print('dataframe complet',lucas_polygons_b.head())\n",
    "        print(lucas_polygons_b.Classif.value_counts())\n",
    "\n",
    "        # Subset the polygons\n",
    "        X_featuresP=lucas_polygons_b.filter(items=['POINT_ID','Classif'])\n",
    "        y_classP=lucas_polygons_b['Classif']#.astype(np.float32)\n",
    "        file.write(\"Input DB polygons shape  : \"+ str(X_featuresP.shape)+\"\\n\") \n",
    "        file.write(\"Input DB polygons columns  : \"+ str(list(X_featuresP.columns))+\"\\n\") \n",
    "    \n",
    "        # 1/ Split between test and train\n",
    "        #TO BE DONE ON THE LUCAS POLYGONS\n",
    "        #https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn\n",
    "        X_trainP,X_testP,y_trainP,y_testP  = train_test_split(X_featuresP,y_classP, test_size=0.2,random_state=5,stratify=y_classP)\n",
    "        file.write(\"X_trainP.shape  : \"+ str(X_trainP.shape)+\"\\n\") \n",
    "        file.write(\"X_testP.shape  : \"+ str(X_testP.shape)+\"\\n\")\n",
    "        file.write(\"y_trainP.shape  : \"+ str(y_trainP.shape)+\"\\n\")\n",
    "        file.write(\"y_testP.shape  : \"+ str(y_testP.shape)+\"\\n\")\n",
    "\n",
    "        # 2/select the pixels from the polygons\n",
    "        #and Subset the DB with regex\n",
    "        indexPOINItrain=pd_lucas_b['POINT_ID'].isin(X_trainP['POINT_ID'])\n",
    "        indexPOINItest=pd_lucas_b['POINT_ID'].isin(X_testP['POINT_ID'])\n",
    "        \n",
    "        X_train=pd_lucas_b[indexPOINItrain].filter(regex=parameters['regex'][i_test])\n",
    "        y_train=pd_lucas_b[indexPOINItrain]['Classif']\n",
    "        X_test=pd_lucas_b[indexPOINItest].filter(regex=parameters['regex'][i_test])\n",
    "        y_test=pd_lucas_b[indexPOINItest]['Classif']\n",
    "        \n",
    "        #write\n",
    "        file.write(\"Input DB X_train pixels shape  : \"+ str(X_train.shape)+\"\\n\") \n",
    "        file.write(\"Input DB X_train pixels columns  : \"+ str(list(X_train.columns))+\"\\n\") \n",
    "        \n",
    "        #keep all info to aggregate prediction per polygons\n",
    "        y_train_pol=pd_lucas_b[indexPOINItrain]\n",
    "        y_test_pol=pd_lucas_b[indexPOINItest]\n",
    "\n",
    "       \n",
    "        # 4/ Save the class distribution for training and testing as CSV\n",
    "        #x = pd.DataFrame(y_train.value_counts().rename_axis('class').reset_index(name='counts'))\n",
    "        x = pd.DataFrame({\"count_pol\": y_train_pol.groupby('POINT_ID').apply(max)['Classif'].value_counts(), \"count_pixel\": y_train.value_counts()}).rename_axis('class')\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_2_Training_class_count_polygons.csv'))\n",
    "        #x = pd.DataFrame(y_test.value_counts().rename_axis('class').reset_index(name='counts'))\n",
    "        x = pd.DataFrame({\"count_pol\": y_test_pol.groupby('POINT_ID').apply(max)['Classif'].value_counts(), \"count_pixel\": y_test.value_counts()}).rename_axis('class')\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_3_Testing_class_count_polygons.csv'))\n",
    "\n",
    "\n",
    "        # 5/ Fit the RANDOM PARAMETERS T\n",
    "        t = time.time()    \n",
    "        clf = RandomForestClassifier(bootstrap=0, criterion='gini', max_depth=None, max_features='auto', \n",
    "                                     min_samples_leaf=12, min_samples_split=3, n_estimators=500, n_jobs=40)\n",
    "                                                                                                                                                                                    \n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        training_time=time.time() - t\n",
    "        file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_1_Processing_Log.txt'),\"a\") \n",
    "        file.write(\"Elapsed time for training  : \"+ str(round(training_time))+\" sec \\n\")\n",
    "        #file.write(\"Model  : \" +str(clf)+\"\\n\")\n",
    "        file.close()\n",
    "\n",
    "        # 6/ Feature importances as  CSV\n",
    "        x = list(zip(clf.feature_importances_,X_train.columns))\n",
    "        x = pd.DataFrame(x,columns=[\"Importance\",\"Feature_Name\"])\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_2_Feature_importance.csv') )\n",
    "        \n",
    "        #append the test value in a file for the 4 biomes\n",
    "        # 7/ OA -evaluate accuracy with the test dataset for the unique rf model\n",
    "        #reclassify the classes by biomes to the simple classes \n",
    "        #transform to series to use replace\n",
    "\n",
    "        #Prediction\n",
    "        y_test_pred=clf.predict(X_test)      \n",
    "        y_test_s=pd.Series(y_test, dtype='float')\n",
    "        y_test_s=y_test_s.replace(classes_classif,classes_classif_simplify)\n",
    "        \n",
    "        y_test_pred_s=pd.Series(y_test_pred, dtype='float')\n",
    "        y_test_pred_s=y_test_pred_s.replace(classes_classif,classes_classif_simplify)\n",
    "                \n",
    "        #to calculate accuracy, go back to array    \n",
    "        accuracy = 100.0*(y_test_s.array == y_test_pred_s.array).sum()/y_test_s.shape[0]\n",
    "        print('Accuracy is :' + str(round(accuracy,2)))\n",
    "    \n",
    "        #del(file)\n",
    "        file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_1_1_Accuracy.txt'),\"w\") \n",
    "        #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "        file1.write(str(accuracy)+\"\\n\") \n",
    "        file1.close()\n",
    "        \n",
    "        # 8/ Classification report\n",
    "        report = classification_report(y_test_s, y_test_pred_s, output_dict=True)\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_3_classification_report.csv') )\n",
    "        \n",
    "        # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "        confusion_mat=confusion_matrix(y_test_s,y_test_pred_s,labels=classes)\n",
    "        confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "        confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_4_confusion_matrix_class.csv'))\n",
    "        \n",
    "        #accuracy mode polygon\n",
    "        #apply a majority rule (mode)\n",
    "        #group it with y_test\n",
    "        y_test_results=pd.DataFrame({'POINT_ID':y_test_pol['POINT_ID'],'ref':y_test,'predict':y_test_pred})\n",
    "        y_test_results=y_test_results.groupby(['POINT_ID'])['predict','ref'].agg(lambda x: x.mode()[0])\n",
    "        \n",
    "        #to calculate accuracy, go back to array    \n",
    "        accuracy_pol = 100.0*(y_test_results['ref'].array == y_test_results['predict'].array).sum()/y_test_results.shape[0]\n",
    "        print('Accuracy is :' + str(round(accuracy_pol,2)))\n",
    "        \n",
    "        #del(file)\n",
    "        file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_1_1_Accuracy_pol.txt'),\"w\") \n",
    "        #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "        file1.write(str(accuracy_pol)+\"\\n\") \n",
    "        file1.close()\n",
    "        # 8/ Classification report\n",
    "        report = classification_report(y_test_results['ref'],y_test_results['predict'], output_dict=True)\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_3_classification_report_pol.csv') )\n",
    "        \n",
    "        # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "        confusion_mat=confusion_matrix(y_test_results['ref'],y_test_results['predict'],labels=classes)\n",
    "        confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "        confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_4_confusion_matrix_class_pol.csv'))\n",
    "        \n",
    "        y_test_s_all=y_test_s_all.append(y_test_s)      \n",
    "        #print(y_test_all)        \n",
    "        y_test_pred_s_all=y_test_pred_s_all.append(y_test_pred_s)\n",
    "        \n",
    "        #pol\n",
    "        y_test_s_all_pol=y_test_s_all_pol.append(y_test_results['ref'])      \n",
    "        #print(y_test_all)        \n",
    "        y_test_pred_s_all_pol=y_test_pred_s_all_pol.append(y_test_results['predict'])\n",
    "        \n",
    "    #to calculate accuracy, go back to array    \n",
    "    accuracy = 100.0*(y_test_s_all.array == y_test_pred_s_all.array).sum()/y_test_s_all.shape[0]\n",
    "    print('Accuracy is :' + str(round(accuracy,2)))\n",
    "   \n",
    "    #del(file)\n",
    "    file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Accuracy.txt'),\"w\") \n",
    "    #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "    file1.write(str(accuracy)+\"\\n\") \n",
    "    file1.close()\n",
    "    \n",
    "    # 8/ Classification report\n",
    "    report = classification_report(y_test_s_all, y_test_pred_s_all, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_3_classification_report.csv') )\n",
    "\n",
    "    # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "    confusion_mat=confusion_matrix(y_test_s_all,y_test_pred_s_all,labels=classes)\n",
    "    confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "    confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_4_confusion_matrix_class.csv'))\n",
    "    #confusion_mat_class=pd.DataFrame(confusion_mat,index= list(labels_s['class']),columns=list(labels_s['class']))\n",
    "    #confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_remap_4_confusion_matrix_class.csv') )\n",
    "    #confusion_mat_label=pd.DataFrame(confusion_mat,index= list(labels_s['label']),columns=list(labels_s['label']))\n",
    "    #confusion_mat_label.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_remap_4_confusion_matrix_label.csv') )\n",
    "\n",
    "    #to calculate accuracy, go back to array    \n",
    "    accuracy_pol = 100.0*(y_test_s_all_pol.array == y_test_pred_s_all_pol.array).sum()/y_test_s_all_pol.shape[0]\n",
    "    print('Accuracy is :' + str(round(accuracy_pol,2)))\n",
    "    \n",
    "    #del(file)\n",
    "    file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Accuracy_pol.txt'),\"w\") \n",
    "    #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "    file1.write(str(accuracy_pol)+\"\\n\") \n",
    "    file1.close()\n",
    "    \n",
    "    # 8/ Classification report\n",
    "    report = classification_report(y_test_s_all_pol, y_test_pred_s_all_pol, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_3_classification_report_pol.csv') )\n",
    "\n",
    "    # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "    confusion_mat=confusion_matrix(y_test_s_all_pol,y_test_pred_s_all_pol,labels=classes)\n",
    "    confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "    confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_4_confusion_matrix_class_pol.csv'))\n",
    "    \n",
    "    file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Processing_Log.txt'),\"a\") \n",
    "    file.write(\"Date and time end: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+\"\\n\") \n",
    "    file.close()\n",
    "    \n",
    "    del(df,clf,confusion_mat)#confusion_mat_label,confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Split on pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manip='DATE-BIOME-STRATIFY-LEVEL1_pix'\n",
    "if not os.path.exists(os.path.join('result',manip)):\n",
    "    os.mkdir(os.path.join('result',manip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
