{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUCAS COPERNICUS\n",
    "# Creation of the random forest models for the Eurocropmap\n",
    "## Land cover mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JEODPP\n",
    "data_path='/eos/jeodpp/data/projects/REFOCUS/data/S1_GS/all-10days/Map_v7/'\n",
    "project_path='/eos/jeodpp/data/projects/REFOCUS/classification/'\n",
    "path_pol = '/eos/jeodpp/data/projects/REFOCUS/data/polygons/v7'\n",
    "results='/eos/jeodpp/data/projects/REFOCUS/classification/'\n",
    "\n",
    "local='/eos/jeodpp/home/users/verheas/data/LUCAS/v7/'\n",
    "\n",
    "# RAF LOCAL\n",
    "# data_path='/data/LUCAS-cop-single-pixel'\n",
    "# project_path='/data/Dropbox/JRC/LANDSENSE/CASE-STUDY-8-LUCAS-COPERNICUS-CLASSIF/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "#import geopandas as gdp\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#pd.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################Parameters##################################################\n",
    "##################################Parameters##################################################\n",
    "#classes - stored in a table 'legend-lucas-all'\n",
    "table_class=pd.read_csv(os.path.join(project_path,'table/legend-lucas-all-v7.csv'),dtype=pd.Int64Dtype())\n",
    "\n",
    "classes_L1=list(table_class['classes_L1'].dropna()) \n",
    "classes_L2=list(table_class['classes_L2'].dropna())\n",
    "\n",
    "#Biome selection\n",
    "\n",
    "biome_1=[1]\n",
    "biome_2=[2]\n",
    "biome_3=[3]\n",
    "biome_4=[4]\n",
    "\n",
    "#level\n",
    "level_1='level_1'\n",
    "level_2='level_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd_lucas (2956889, 116)\n",
      "300    1216530\n",
      "200    1000318\n",
      "500     732964\n",
      "600       3856\n",
      "100       3221\n",
      "Name: level_1, dtype: int64\n",
      "300    1216530\n",
      "500     732964\n",
      "211     290116\n",
      "213     142886\n",
      "216     125644\n",
      "232      64452\n",
      "290      63609\n",
      "250      59053\n",
      "214      35215\n",
      "231      34577\n",
      "240      34067\n",
      "215      34005\n",
      "212      28825\n",
      "222      23174\n",
      "218      19274\n",
      "221      15815\n",
      "230      12000\n",
      "233       7094\n",
      "223       4920\n",
      "219       4724\n",
      "600       3856\n",
      "100       3221\n",
      "217        868\n",
      "Name: level_2, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (27,30,33,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>POINT_ID</th>\n",
       "      <th>NUTS0</th>\n",
       "      <th>NUTS1</th>\n",
       "      <th>NUTS2</th>\n",
       "      <th>NUTS3</th>\n",
       "      <th>TH_LAT</th>\n",
       "      <th>TH_LONG</th>\n",
       "      <th>OFFICE_PI</th>\n",
       "      <th>EX_ANTE</th>\n",
       "      <th>...</th>\n",
       "      <th>LU2_LABEL</th>\n",
       "      <th>LU1_TYPE_LABEL</th>\n",
       "      <th>LU2_TYPE_LABEL</th>\n",
       "      <th>CPRN_LC_LABEL</th>\n",
       "      <th>CPRN_LC_SAME_LC1</th>\n",
       "      <th>LUCAS_CORE_INTERSECT</th>\n",
       "      <th>COPERNICUS_CLEANED</th>\n",
       "      <th>stratum</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34562080</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES2</td>\n",
       "      <td>ES24</td>\n",
       "      <td>ES243</td>\n",
       "      <td>41.288386</td>\n",
       "      <td>-0.319428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other bare soil</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>290</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31243520</td>\n",
       "      <td>IE</td>\n",
       "      <td>IE0</td>\n",
       "      <td>IE04</td>\n",
       "      <td>IE042</td>\n",
       "      <td>53.422977</td>\n",
       "      <td>-8.226052</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spontaneously re-vegetated surfaces</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>33661774</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES5</td>\n",
       "      <td>ES52</td>\n",
       "      <td>ES521</td>\n",
       "      <td>38.434388</td>\n",
       "      <td>-0.905705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Permanent crops: fruit trees</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>28922250</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES1</td>\n",
       "      <td>ES11</td>\n",
       "      <td>ES113</td>\n",
       "      <td>41.867145</td>\n",
       "      <td>-7.320304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shrubland with sparse tree cover</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>35082906</td>\n",
       "      <td>FR</td>\n",
       "      <td>FRD</td>\n",
       "      <td>FRD1</td>\n",
       "      <td>FRD12</td>\n",
       "      <td>48.715190</td>\n",
       "      <td>-1.092190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Grassland without tree/shrub cover</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  POINT_ID NUTS0 NUTS1 NUTS2  NUTS3     TH_LAT   TH_LONG  \\\n",
       "0           0  34562080    ES   ES2  ES24  ES243  41.288386 -0.319428   \n",
       "1           1  31243520    IE   IE0  IE04  IE042  53.422977 -8.226052   \n",
       "2           2  33661774    ES   ES5  ES52  ES521  38.434388 -0.905705   \n",
       "3           3  28922250    ES   ES1  ES11  ES113  41.867145 -7.320304   \n",
       "4           4  35082906    FR   FRD  FRD1  FRD12  48.715190 -1.092190   \n",
       "\n",
       "   OFFICE_PI  EX_ANTE  ...     LU2_LABEL  LU1_TYPE_LABEL  LU2_TYPE_LABEL  \\\n",
       "0          0        0  ...  Not relevant             NaN             NaN   \n",
       "1          0        0  ...  Not relevant             NaN             NaN   \n",
       "2          0        0  ...  Not relevant             NaN             NaN   \n",
       "3          0        0  ...  Not relevant             NaN             NaN   \n",
       "4          0        0  ...  Not relevant             NaN             NaN   \n",
       "\n",
       "                         CPRN_LC_LABEL  CPRN_LC_SAME_LC1  \\\n",
       "0                      Other bare soil              True   \n",
       "1  Spontaneously re-vegetated surfaces              True   \n",
       "2         Permanent crops: fruit trees              True   \n",
       "3     Shrubland with sparse tree cover              True   \n",
       "4   Grassland without tree/shrub cover              True   \n",
       "\n",
       "   LUCAS_CORE_INTERSECT  COPERNICUS_CLEANED  stratum  level_2  level_1  \n",
       "0                  True                True        2      290      200  \n",
       "1                  True                True        1      500      500  \n",
       "2                  True                True        2      300      300  \n",
       "3                  True                True        2      300      300  \n",
       "4                  True                True        1      500      500  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the data\n",
    "#1) load the S1 10 days extracted values in GEE for all polygons\n",
    "\n",
    "pd_lucas= pd.read_csv(os.path.join(data_path,'S1_point_allV7_10days_10m_1Jan-31Dec_EU_ratio-db.csv'),dtype={'level_1':int,'level_2':int})\n",
    "print('pd_lucas',pd_lucas.shape)\n",
    "\n",
    "#concatenate all the data in one dataframe\n",
    "#group cropland, grassland and bareland \n",
    "#number of pixels per class\n",
    "print(pd_lucas.level_1.value_counts())\n",
    "print(pd_lucas.level_2.value_counts())\n",
    "pd_lucas.head()\n",
    "\n",
    "#number of pixels per class\n",
    "#pd_lucas.LC1_COD.value_counts()\n",
    "#pd_lucas.head()\n",
    "pd_lucas.columns\n",
    "\n",
    "##############1.2 Load the shapefile with the polygons - useful to split the polygons in training and test dataset for the accuracy ######################\n",
    "# load csv with of the polygons\n",
    "#2)load csv with the polygons for the split test/validation\n",
    "lucas_polygons = pd.read_csv(os.path.join(path_pol,'LUCAS_2018_Copernicus_attributes_cropmap_level1-2.csv'))\n",
    "lucas_polygons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biome 1 - parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biome [1]\n",
      "level level_1\n",
      "level [100, 200, 300, 500, 600]\n"
     ]
    }
   ],
   "source": [
    "###################################Choose parameters for this run #############################################\n",
    "#Biome\n",
    "biome=biome_1\n",
    "print('biome',biome)\n",
    "\n",
    "#level\n",
    "level=level_1\n",
    "print('level',level)\n",
    "#crop - level 2, from the table we load only the crop type classes\n",
    "classes=classes_L1\n",
    "print('level',classes)\n",
    "\n",
    "#Split for the train/test dataset - we run it with all the polygons\n",
    "#split_test = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome 1 - Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2956889, 117)\n",
      "pd_lucas_b 300    932467\n",
      "200    813124\n",
      "500    624627\n",
      "600      2488\n",
      "100      2449\n",
      "Name: Classif, dtype: int64\n",
      "pd_lucas_b 300    1216530\n",
      "200    1000318\n",
      "500     732964\n",
      "600       3856\n",
      "100       3221\n",
      "Name: Classif, dtype: int64\n",
      "1    43898\n",
      "Name: stratum, dtype: int64\n",
      "X_train head     VH_20180101  VH_20180111  VH_20180121  VH_20180131  VH_20180210  \\\n",
      "16   -15.841164   -16.101488   -16.373434   -15.005488   -17.550755   \n",
      "17   -14.036120   -14.616349   -14.647615   -14.757492   -15.923586   \n",
      "18   -14.036120   -15.017238   -13.691375   -13.207221   -15.077690   \n",
      "19   -10.934003   -10.906717   -10.008709   -12.063789   -12.230721   \n",
      "20   -11.434500   -11.976395   -11.637123   -12.429725   -12.179104   \n",
      "\n",
      "    VH_20180220  VH_20180302  VH_20180312  VH_20180322  VH_20180401  ...  \\\n",
      "16   -16.679602   -17.511936   -17.628834   -16.359150   -13.442070  ...   \n",
      "17   -16.752005   -17.511936   -17.170132   -15.328428   -11.951139  ...   \n",
      "18   -16.350103   -16.916924   -15.727412   -15.328428   -12.121370  ...   \n",
      "19   -11.193401    -9.686357   -11.658030   -11.937874   -10.483664  ...   \n",
      "20   -12.411910   -11.339608   -12.429645   -14.114943   -11.006748  ...   \n",
      "\n",
      "    VV_20180501  VV_20180511  VV_20180521  VV_20180531  VV_20180610  \\\n",
      "16    -9.864964    -8.526443    -7.950086    -8.436173    -9.266834   \n",
      "17    -5.856219    -5.080046    -7.153901    -6.093048    -6.469603   \n",
      "18    -5.650132    -4.457147    -5.634888    -5.335476    -6.325647   \n",
      "19    -7.545943    -9.836915    -7.080260    -7.629286    -7.964881   \n",
      "20    -8.475918   -10.688531    -7.770342    -7.508136    -8.492286   \n",
      "\n",
      "    VV_20180620  VV_20180630  VV_20180710  VV_20180720  VV_20180730  \n",
      "16    -7.866013    -7.928831    -9.250545    -8.344183    -8.255922  \n",
      "17    -6.487081    -6.717664    -7.169580    -8.109546    -8.022132  \n",
      "18    -6.725405    -6.172128    -6.259192    -6.066954    -5.761969  \n",
      "19    -8.511303    -7.313365    -7.784080    -7.418647    -7.221469  \n",
      "20    -7.773233    -8.147706    -7.783479    -7.166018    -8.199863  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "X_train shape (2375155, 44)\n",
      "y_train shape (2375155,)\n",
      "y_train count 300    932467\n",
      "200    813124\n",
      "500    624627\n",
      "600      2488\n",
      "100      2449\n",
      "Name: Classif, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#### 2) Prepare the data for the classification ##############\n",
    "##############################################################\n",
    "\n",
    "#############2.1 Select level of work and classes\n",
    "#copy values in a new column 'Classif' that we will use in the rest of the script\n",
    "pd_lucas['Classif']=pd_lucas[level]\n",
    "print(pd_lucas.shape)\n",
    "\n",
    "pd_lucas_i=pd_lucas[pd_lucas.Classif.isin(classes_L1)]\n",
    "\n",
    "#############2.2 Select the biome\n",
    "#select biome\n",
    "pd_lucas_b=pd_lucas_i[pd_lucas_i.stratum.isin(biome)]\n",
    "print('pd_lucas_b',pd_lucas_b.Classif.value_counts())\n",
    "print('pd_lucas_b',pd_lucas.Classif.value_counts())\n",
    "print(pd_lucas_b.groupby('POINT_ID').apply(min).stratum.value_counts())\n",
    "\n",
    "#############2.3 Select the data inputs for the classification\n",
    "## we use all the polygons, therefore there are no test dataset\n",
    "\n",
    "X_train=pd_lucas_b.filter(regex='(((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-7])')\n",
    "y_train=pd_lucas_b['Classif']\n",
    "\n",
    "print('X_train head',X_train.head())\n",
    "print('X_train shape',X_train.shape)\n",
    "\n",
    "print('y_train shape',y_train.shape)\n",
    "print('y_train count',y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/eos/jeodpp/data/projects/REFOCUS/classification/RFmodel_LUCAS_[1]_level_1_all-polygons_janv-jul2018_15122020b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mOSError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9c1c35538bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                              min_samples_leaf=4, min_samples_split=3, n_estimators=800, n_jobs=40)\n\u001b[1;32m      3\u001b[0m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RFmodel_LUCAS_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiome\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_all-polygons_janv-jul2018_15122020b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/eos/jeodpp/data/projects/REFOCUS/classification/RFmodel_LUCAS_[1]_level_1_all-polygons_janv-jul2018_15122020b'"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(bootstrap=0, criterion='gini', max_depth=None, max_features='sqrt', \n",
    "                             min_samples_leaf=4, min_samples_split=3, n_estimators=800, n_jobs=40)\n",
    "rfc.fit(X_train.values, y_train.values)\n",
    "pickle.dump(rfc, open(os.path.join(project_path,'RFmodel_LUCAS_'+str(biome)+'_'+str(level)+'_all-polygons_janv-jul2018_15122020b'), 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome 1 - Run a RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9bcd12222560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Grid Search for a set of parameters - without scaler - no need of pipeline?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#there is also the randomized search CV that will not run all the possibilities - we can use to get a first idea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mMethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Random Forest'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Method:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "#Grid Search for a set of parameters - without scaler - no need of pipeline?\n",
    "#there is also the randomized search CV that will not run all the possibilities - we can use to get a first idea\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = { \n",
    "    'RFclf__n_estimators': [150,180,210,240,270,300,500,800,900,1000,1100,1200], #The number of trees in the forest.\n",
    "    'RFclf__max_features': ['auto', 'sqrt', 'log2'], #The number of features to consider when looking for the best split:\n",
    "    'RFclf__max_depth': [4,5,6,8,12,None],#The maximum depth of the tree\n",
    "    'RFclf__min_samples_leaf': [1,2,3,4,8,10,12],#The minimum number of samples in newly created leaves.\n",
    "    'RFclf__bootstrap': [0, 1],#Whether bootstrap samples are used when building trees.\n",
    "    'RFclf__min_samples_split': [3, 5, 7], #The minimum number of samples required to split an internal node\n",
    "    'RFclf__criterion': ['gini', 'entropy']#The function to measure the quality of a split\n",
    "}\n",
    "#Scaler = preprocessing.StandardScaler()\n",
    "#Scaler.fit(TrF)\n",
    "Pipeline = Pipeline([('RFclf', RandomForestClassifier())])\n",
    "#rfc = GridSearchCV(estimator=Pipeline, param_grid=param_grid,cv=3, n_jobs=100, verbose=1)\n",
    "rfc = RandomizedSearchCV(estimator=Pipeline, param_distributions =param_grid, n_iter=100,cv=3, n_jobs=20, verbose=1)\n",
    "#RandomizedSearchCV\n",
    "#rfc.fit(Scaler.transform(TrF), TrC)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "\n",
    "\n",
    "pickle.dump(rfc, open('RFmodel_LUCAS_'+str(biome)+'_'+str(level)+'_all-polygons_janv-jul2018_15122020best', 'wb'))\n",
    "\n",
    "print(rfc.best_estimator_.named_steps['RFclf'])\n",
    "\n",
    "#best_model=rfc.best_estimator_.named_steps['RFclf']\n",
    "\n",
    "#best model\n",
    "#print('best estimator',best_model)\n",
    "\n",
    "#pickle.dump(best_model, open('RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-jul2018_160920-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfc.best_estimator_.named_steps['RFclf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "phat = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biome 2 - parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biome [2]\n",
      "level level_1\n",
      "level [100, 200, 300, 500, 600]\n"
     ]
    }
   ],
   "source": [
    "###################################Choose parameters for this run #############################################\n",
    "#Biome\n",
    "biome=biome_2\n",
    "print('biome',biome)\n",
    "\n",
    "#level\n",
    "level=level_1\n",
    "print('level',level)\n",
    "#crop - level 2, from the table we load only the crop type classes\n",
    "classes=classes_L1\n",
    "print('level',classes)\n",
    "\n",
    "#Split for the train/test dataset - we run it with all the polygons\n",
    "#split_test = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome 2 - Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2956889, 117)\n",
      "pd_lucas_b 300    284063\n",
      "200    187194\n",
      "500    108337\n",
      "600      1368\n",
      "100       772\n",
      "Name: Classif, dtype: int64\n",
      "pd_lucas_b 300    1216530\n",
      "200    1000318\n",
      "500     732964\n",
      "600       3856\n",
      "100       3221\n",
      "Name: Classif, dtype: int64\n",
      "2    14280\n",
      "Name: stratum, dtype: int64\n",
      "X_train head    VH_20180101  VH_20180111  VH_20180121  VH_20180131  VH_20180210  \\\n",
      "0   -16.954021   -16.242271   -15.693783   -18.552937   -17.400238   \n",
      "1   -17.597748   -18.795210   -18.761711   -19.685406   -18.706253   \n",
      "2   -16.457218   -18.847889   -17.879326   -19.397968   -17.177315   \n",
      "3   -15.408525   -17.031052   -15.888536   -17.946339   -15.941470   \n",
      "4   -12.962501   -14.945486   -13.482009   -16.617935   -15.778416   \n",
      "\n",
      "   VH_20180220  VH_20180302  VH_20180312  VH_20180322  VH_20180401  ...  \\\n",
      "0   -16.954105   -16.281940   -15.294356   -16.696960   -17.772724  ...   \n",
      "1   -16.822638   -17.532448   -19.449732   -18.291378   -19.510078  ...   \n",
      "2   -16.013180   -17.093063   -17.248064   -17.229742   -17.303719  ...   \n",
      "3   -15.348351   -15.851156   -15.673327   -15.637535   -15.842174  ...   \n",
      "4   -14.695146   -14.990697   -14.436112   -15.228673   -15.064209  ...   \n",
      "\n",
      "   VV_20180501  VV_20180511  VV_20180521  VV_20180531  VV_20180610  \\\n",
      "0   -13.596285   -12.193822   -12.923239   -11.018482   -12.144523   \n",
      "1   -10.855453   -12.358705    -9.834004   -11.639806   -12.461661   \n",
      "2   -10.710681   -11.205153   -11.065721   -11.399288   -10.323773   \n",
      "3    -7.641057   -10.611853    -9.880006    -6.349081    -7.520484   \n",
      "4    -4.545300    -7.176936    -6.445380    -6.349081    -5.599817   \n",
      "\n",
      "   VV_20180620  VV_20180630  VV_20180710  VV_20180720  VV_20180730  \n",
      "0   -11.312380   -12.462443   -10.578107   -10.788980   -11.958767  \n",
      "1   -13.239387   -13.167953   -11.477935   -12.520679   -10.858428  \n",
      "2   -11.792188   -12.187102   -10.383499   -11.730686    -9.652574  \n",
      "3    -9.495626    -6.981897    -8.017899    -9.762844    -6.712222  \n",
      "4    -7.205727    -4.459853    -4.450773    -5.922249    -4.499658  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "X_train shape (581734, 44)\n",
      "y_train shape (581734,)\n",
      "y_train count 300    284063\n",
      "200    187194\n",
      "500    108337\n",
      "600      1368\n",
      "100       772\n",
      "Name: Classif, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#### 2) Prepare the data for the classification ##############\n",
    "##############################################################\n",
    "\n",
    "#############2.1 Select level of work and classes\n",
    "#copy values in a new column 'Classif' that we will use in the rest of the script\n",
    "pd_lucas['Classif']=pd_lucas[level]\n",
    "print(pd_lucas.shape)\n",
    "\n",
    "pd_lucas_i=pd_lucas[pd_lucas.Classif.isin(classes)]\n",
    "\n",
    "#############2.2 Select the biome\n",
    "#select biome\n",
    "pd_lucas_b=pd_lucas_i[pd_lucas_i.stratum.isin(biome)]\n",
    "print('pd_lucas_b',pd_lucas_b.Classif.value_counts())\n",
    "print('pd_lucas_b',pd_lucas.Classif.value_counts())\n",
    "print(pd_lucas_b.groupby('POINT_ID').apply(min).stratum.value_counts())\n",
    "\n",
    "#############2.3 Select the data inputs for the classification\n",
    "## we use all the polygons, therefore there are no test dataset\n",
    "\n",
    "X_train=pd_lucas_b.filter(regex='(((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-7])')\n",
    "y_train=pd_lucas_b['Classif']\n",
    "\n",
    "print('X_train head',X_train.head())\n",
    "print('X_train shape',X_train.shape)\n",
    "\n",
    "print('y_train shape',y_train.shape)\n",
    "print('y_train count',y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome 2 - Run a RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Random Forest\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=38)]: Using backend LokyBackend with 38 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "#Grid Search for a set of parameters - without scaler - no need of pipeline?\n",
    "#there is also the randomized search CV that will not run all the possibilities - we can use to get a first idea\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = { \n",
    "    'RFclf__n_estimators': [150,180,210,240,270,300,500,800,900,1000,1100,1200],\n",
    "    'RFclf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'RFclf__max_depth': [4,5,6,8,12,None],\n",
    "    'RFclf__min_samples_leaf': [1,2,3,4,8,10,12],\n",
    "    'RFclf__bootstrap': [0, 1],\n",
    "    'RFclf__min_samples_split': [3, 5, 7],\n",
    "    'RFclf__criterion': ['gini', 'entropy']\n",
    "}\n",
    "#Scaler = preprocessing.StandardScaler()\n",
    "#Scaler.fit(TrF)\n",
    "Pipeline = Pipeline([('RFclf', RandomForestClassifier())])\n",
    "#rfc = GridSearchCV(estimator=Pipeline, param_grid=param_grid,cv=3, n_jobs=100, verbose=1)\n",
    "rfc = RandomizedSearchCV(estimator=Pipeline, param_distributions =param_grid, n_iter=100,cv=3, n_jobs=38, verbose=1)\n",
    "#RandomizedSearchCV\n",
    "#rfc.fit(Scaler.transform(TrF), TrC)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "\n",
    "\n",
    "pickle.dump(rfc, open('RFmodel_LUCAS_'+str(biome)+'_'+str(level)+'_all-polygons_janv-jul2018_15122020best', 'wb'))\n",
    "\n",
    "print(rfc.best_estimator_.named_steps['RFclf'])\n",
    "\n",
    "#best_model=rfc.best_estimator_.named_steps['RFclf']\n",
    "\n",
    "#best model\n",
    "#print('best estimator',best_model)\n",
    "\n",
    "#pickle.dump(best_model, open('RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-jul2018_160920-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biome 3 - parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################Choose parameters#############################################\n",
    "classes=classes_L1\n",
    "\n",
    "#reclassification\n",
    "classes_in=classes_in_L1\n",
    "print ('classes_in',classes_in)\n",
    "\n",
    "#classes_remap=classes_remap_L1\n",
    "#print ('classes_remap',classes_remap)\n",
    "\n",
    "#classes for the classification and biome/no biome differentiation if needed\n",
    "classes_B=classes_L1_B\n",
    "print ('classes_B',classes_B)\n",
    "\n",
    "classes_NB=classes_L1_NB\n",
    "print ('classes_NB',classes_NB)\n",
    "\n",
    "#summary of the classses used for the classification\n",
    "classes_classif=classes_classif_L1\n",
    "print ('classes_classif',classes_classif)\n",
    "classes_classif_simplify=classes_classif_L1_simplify\n",
    "print ('classes_classif_simplify',classes_classif_simplify)\n",
    "\n",
    "#Biome\n",
    "biome=biome_3\n",
    "print('biome',biome)\n",
    "\n",
    "#level\n",
    "level=level_1\n",
    "print('level',level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome 3 - Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#### 2) Prepare the data for the classification ##############\n",
    "##############################################################\n",
    "\n",
    "#############2.1 Reclassify\n",
    "#copy values in a new column 'Classif' that we will use in the rest of the script\n",
    "pd_lucas_i=pd_lucas\n",
    "\n",
    "pd_lucas_i['Classif']=pd_lucas_i[level]\n",
    "pd_lucas_i.head()\n",
    "#print(pd_lucas.Classif.value_counts())\n",
    "\n",
    "#reclassification of the classes according to the level\n",
    "#pd_lucas.Classif=pd_lucas.Classif.replace(classes_in,\n",
    "#                                            classes_remap)\n",
    "print(pd_lucas_i.shape)\n",
    "#print(pd_lucas.Classif.value_counts())\n",
    "\n",
    "#############2.2 Select the biome\n",
    "#create a column with the value of the class + the biome (add the biome after the class)\n",
    "#for the level-1 class, the \n",
    "pd_lucas_biome=pd_lucas_i[pd_lucas_i.Classif.isin(classes_B)]\n",
    "pd_lucas_nobiome=pd_lucas_i[pd_lucas_i.Classif.isin(classes_NB)]\n",
    "\n",
    "pd_lucas_biome['ClassifB']=pd_lucas_biome['Classif'].astype(str) + pd_lucas_biome['stratum'].astype(str)\n",
    "pd_lucas_nobiome['ClassifB']=pd_lucas_nobiome['Classif'].astype(str) + '0'\n",
    "\n",
    "#select biome\n",
    "pd_lucas_biome=pd_lucas_biome[pd_lucas_biome.stratum.isin(biome)]\n",
    "pd_lucas_b=pd_lucas_biome.append(pd_lucas_nobiome)\n",
    "\n",
    "#print(pd_lucas_b.head())\n",
    "print('pd_lucas_b',pd_lucas_b.ClassifB.value_counts())\n",
    "print('pd_lucas_b',pd_lucas_b.Classif.value_counts())\n",
    "\n",
    "#############2.3 Select the data inputs for the classification\n",
    "## we use all the polygons, therefore there are no test dataset\n",
    "\n",
    "X_train=pd_lucas_b.filter(regex='(((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-7])')\n",
    "y_train=pd_lucas_b['Classif']\n",
    "\n",
    "print('X_train head',X_train.head())\n",
    "print('X_train shape',X_train.shape)\n",
    "\n",
    "print('y_train shape',y_train.shape)\n",
    "print('y_train count',y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome 3 - Run a RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search for a set of parameters - without scaler - no need of pipeline?\n",
    "#there is also the randomized search CV that will not run all the possibilities - we can use to get a first idea\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = { \n",
    "    'RFclf__n_estimators': [50,75,100,130,150,180,210,240,270],\n",
    "    'RFclf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'RFclf__max_depth': [4,5,6,8,12,None],\n",
    "    'RFclf__min_samples_leaf': [1,2,3,4,8,10,12],\n",
    "    'RFclf__bootstrap': [0, 1],\n",
    "    'RFclf__min_samples_split': [3, 5, 7],\n",
    "    'RFclf__criterion': ['gini', 'entropy']\n",
    "}\n",
    "#Scaler = preprocessing.StandardScaler()\n",
    "#Scaler.fit(TrF)\n",
    "Pipeline = Pipeline([('RFclf', RandomForestClassifier())])\n",
    "#rfc = GridSearchCV(estimator=Pipeline, param_grid=param_grid,cv=3, n_jobs=100, verbose=1)\n",
    "rfc = RandomizedSearchCV(estimator=Pipeline, param_distributions =param_grid, n_iter=100,cv=3, n_jobs=38, verbose=1)\n",
    "#RandomizedSearchCV\n",
    "#rfc.fit(Scaler.transform(TrF), TrC)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "\n",
    "\n",
    "pickle.dump(rfc, open('RFmodel_LUCAS_'+str(biome)+'_'+str(level)+'_all-polygons_janv-jul2018_17092020-bis', 'wb'))\n",
    "\n",
    "print(rfc.best_estimator_.named_steps['RFclf'])\n",
    "\n",
    "#best_model=rfc.best_estimator_.named_steps['RFclf']\n",
    "\n",
    "#best model\n",
    "#print('best estimator',best_model)\n",
    "\n",
    "#pickle.dump(best_model, open('RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-jul2018_160920-best', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biome 4 - parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################Choose parameters#############################################\n",
    "classes=classes_L1\n",
    "\n",
    "#reclassification\n",
    "classes_in=classes_in_L1\n",
    "print ('classes_in',classes_in)\n",
    "\n",
    "#classes_remap=classes_remap_L1\n",
    "#print ('classes_remap',classes_remap)\n",
    "\n",
    "#classes for the classification and biome/no biome differentiation if needed\n",
    "classes_B=classes_L1_B\n",
    "print ('classes_B',classes_B)\n",
    "\n",
    "classes_NB=classes_L1_NB\n",
    "print ('classes_NB',classes_NB)\n",
    "\n",
    "#summary of the classses used for the classification\n",
    "classes_classif=classes_classif_L1\n",
    "print ('classes_classif',classes_classif)\n",
    "classes_classif_simplify=classes_classif_L1_simplify\n",
    "print ('classes_classif_simplify',classes_classif_simplify)\n",
    "\n",
    "#Biome\n",
    "biome=biome_4\n",
    "print('biome',biome)\n",
    "\n",
    "#level\n",
    "level=level_1\n",
    "print('level',level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome 4 - Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#### 2) Prepare the data for the classification ##############\n",
    "##############################################################\n",
    "\n",
    "#############2.1 Reclassify\n",
    "#copy values in a new column 'Classif' that we will use in the rest of the script\n",
    "pd_lucas_i=pd_lucas\n",
    "\n",
    "pd_lucas_i['Classif']=pd_lucas_i[level]\n",
    "pd_lucas_i.head()\n",
    "#print(pd_lucas.Classif.value_counts())\n",
    "\n",
    "#reclassification of the classes according to the level\n",
    "#pd_lucas.Classif=pd_lucas.Classif.replace(classes_in,\n",
    "#                                            classes_remap)\n",
    "print(pd_lucas_i.shape)\n",
    "#print(pd_lucas.Classif.value_counts())\n",
    "\n",
    "#############2.2 Select the biome\n",
    "#create a column with the value of the class + the biome (add the biome after the class)\n",
    "#for the level-1 class, the \n",
    "pd_lucas_biome=pd_lucas_i[pd_lucas_i.Classif.isin(classes_B)]\n",
    "pd_lucas_nobiome=pd_lucas_i[pd_lucas_i.Classif.isin(classes_NB)]\n",
    "\n",
    "pd_lucas_biome['ClassifB']=pd_lucas_biome['Classif'].astype(str) + pd_lucas_biome['stratum'].astype(str)\n",
    "pd_lucas_nobiome['ClassifB']=pd_lucas_nobiome['Classif'].astype(str) + '0'\n",
    "\n",
    "#select biome\n",
    "pd_lucas_biome=pd_lucas_biome[pd_lucas_biome.stratum.isin(biome)]\n",
    "pd_lucas_b=pd_lucas_biome.append(pd_lucas_nobiome)\n",
    "\n",
    "#print(pd_lucas_b.head())\n",
    "print('pd_lucas_b',pd_lucas_b.ClassifB.value_counts())\n",
    "print('pd_lucas_b',pd_lucas_b.Classif.value_counts())\n",
    "\n",
    "#############2.3 Select the data inputs for the classification\n",
    "## we use all the polygons, therefore there are no test dataset\n",
    "\n",
    "X_train=pd_lucas_b.filter(regex='(((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-7])')\n",
    "y_train=pd_lucas_b['Classif']\n",
    "\n",
    "print('X_train head',X_train.head())\n",
    "print('X_train shape',X_train.shape)\n",
    "\n",
    "print('y_train shape',y_train.shape)\n",
    "print('y_train count',y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome 4 - Run a RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search for a set of parameters - without scaler - no need of pipeline?\n",
    "#there is also the randomized search CV that will not run all the possibilities - we can use to get a first idea\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = { \n",
    "    'RFclf__n_estimators': [50,75,100,130,150,180,210,240,270],\n",
    "    'RFclf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'RFclf__max_depth': [4,5,6,8,12,None],\n",
    "    'RFclf__min_samples_leaf': [1,2,3,4,8,10,12],\n",
    "    'RFclf__bootstrap': [0, 1],\n",
    "    'RFclf__min_samples_split': [3, 5, 7],\n",
    "    'RFclf__criterion': ['gini', 'entropy']\n",
    "}\n",
    "#Scaler = preprocessing.StandardScaler()\n",
    "#Scaler.fit(TrF)\n",
    "Pipeline = Pipeline([('RFclf', RandomForestClassifier())])\n",
    "#rfc = GridSearchCV(estimator=Pipeline, param_grid=param_grid,cv=3, n_jobs=100, verbose=1)\n",
    "rfc = RandomizedSearchCV(estimator=Pipeline, param_distributions =param_grid, n_iter=100,cv=3, n_jobs=38, verbose=1)\n",
    "#RandomizedSearchCV\n",
    "#rfc.fit(Scaler.transform(TrF), TrC)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "\n",
    "\n",
    "pickle.dump(rfc, open('RFmodel_LUCAS_'+str(biome)+'_'+str(level)+'_all-polygons_janv-jul2018_17092020-bis', 'wb'))\n",
    "\n",
    "print(rfc.best_estimator_.named_steps['RFclf'])\n",
    "\n",
    "#best_model=rfc.best_estimator_.named_steps['RFclf']\n",
    "\n",
    "#best model\n",
    "#print('best estimator',best_model)\n",
    "\n",
    "#pickle.dump(best_model, open('RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-jul2018_160920-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search for a set of parameters - takes time \n",
    "#there is also the randomized search CV that will not run all the possibilities - we can use to get a first idea\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = { \n",
    "    'RFclf__n_estimators': [50,75,100,130,150,180],\n",
    "    'RFclf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'RFclf__max_depth': [4,5,6,8,12,None],\n",
    "    'RFclf__min_samples_leaf': [1,2,3,4,8,10,12],\n",
    "    'RFclf__bootstrap': [0, 1],\n",
    "    'RFclf__min_samples_split': [3, 5, 7],\n",
    "    'RFclf__criterion': ['gini', 'entropy']\n",
    "}\n",
    "#Scaler = preprocessing.StandardScaler()\n",
    "#Scaler.fit(TrF)\n",
    "Pipeline = Pipeline([('Scaler', preprocessing.StandardScaler()), ('RFclf', RandomForestClassifier())])\n",
    "#rfc = GridSearchCV(estimator=Pipeline, param_grid=param_grid,cv=3, n_jobs=100, verbose=1)\n",
    "rfc = RandomizedSearchCV(estimator=Pipeline, param_distributions =param_grid, n_iter=100,cv=3, n_jobs=38, verbose=1)\n",
    "#RandomizedSearchCV\n",
    "#rfc.fit(Scaler.transform(TrF), TrC)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "\n",
    "\n",
    "pickle.dump(rfc, open('RFmodel_LUCAS_'+str(biome)+str(level)+'_all-polygons_janv-jul2018_17092020', 'wb'))\n",
    "\n",
    "print(rfc.best_estimator_.named_steps['RFclf'])\n",
    "\n",
    "#best model\n",
    "#print('best estimator',best_model)\n",
    "\n",
    "#pickle.dump(best_model, open('RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-jul2018_160920-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=0, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=7,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=180, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "classifier = rfc.best_estimator_.named_steps['RFclf']\n",
    "print (classifier)\n",
    "pickle.dump(classifier, open('RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-jul2018_160920-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=0, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=7,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=180, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(rfc.best_estimator_.steps[1][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST FOR ONE SET OF PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Random Forest\n",
      "Elapsed time for training: 141.43 sec\n",
      "Accuracy is :94.93\n",
      "[[89168   221  2963     0     0]\n",
      " [  605 76324  2364     0     1]\n",
      " [ 3819  1342 55756     0     1]\n",
      " [  135    66   119    56     0]\n",
      " [   38    78    70     0    42]]\n"
     ]
    }
   ],
   "source": [
    "#TEST WITH RANDOM PARAMETERS TO TEST FEATURES IMPORTANCE\n",
    "#July\n",
    "#split test and validation\n",
    "X_trainP,X_testP,y_trainP,y_testP  = train_test_split(X_train,y_train, test_size=0.25,random_state=5)#,stratify=[200,300,500,600,100])\n",
    "\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(bootstrap=0, criterion='gini', max_depth=None, max_features='sqrt', \n",
    "                             min_samples_leaf=4, min_samples_split=3, n_estimators=150, n_jobs=40)\n",
    "\n",
    "rfc.fit(X_trainP.values, y_trainP.values)\n",
    "\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "#pickle.dump(clf, open('RFmodel_1x1_scenario2_test', 'wb'))\n",
    "\n",
    "\n",
    "#accuracy\n",
    "y_test_pred=rfc.predict(X_testP)      \n",
    "y_test_pred_s=pd.Series(y_test_pred, dtype='float')\n",
    "        \n",
    "#to calculate accuracy, go back to array    \n",
    "accuracy = 100.0*(y_testP.array == y_test_pred_s.array).sum()/X_testP.shape[0]\n",
    "print('Accuracy is :' + str(round(accuracy,2)))\n",
    "\n",
    "confusion_mat=confusion_matrix(y_testP,y_test_pred_s,labels=list([200,300,500,600,100]))\n",
    "print (confusion_mat)\n",
    "import pickle\n",
    "pickle.dump(rfc, open('RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-juil2018bis', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        200\n",
      "1        200\n",
      "2        200\n",
      "3        200\n",
      "4        200\n",
      "5        200\n",
      "6        200\n",
      "7        200\n",
      "8        200\n",
      "9        200\n",
      "10       200\n",
      "11       200\n",
      "12       200\n",
      "13       200\n",
      "14       200\n",
      "15       200\n",
      "16       200\n",
      "17       200\n",
      "18       200\n",
      "19       200\n",
      "20       200\n",
      "21       200\n",
      "22       200\n",
      "23       200\n",
      "24       200\n",
      "25       200\n",
      "26       200\n",
      "27       200\n",
      "28       200\n",
      "29       200\n",
      "        ... \n",
      "30244    600\n",
      "30245    600\n",
      "30246    600\n",
      "30247    600\n",
      "30248    600\n",
      "30249    600\n",
      "30250    600\n",
      "30251    600\n",
      "30252    600\n",
      "30253    600\n",
      "30254    600\n",
      "30255    600\n",
      "30256    600\n",
      "30257    600\n",
      "30258    600\n",
      "30259    600\n",
      "30260    600\n",
      "30261    600\n",
      "30262    600\n",
      "30263    600\n",
      "30264    600\n",
      "30265    600\n",
      "30266    600\n",
      "30267    600\n",
      "30268    600\n",
      "30269    600\n",
      "30270    600\n",
      "30271    600\n",
      "30272    600\n",
      "30273    600\n",
      "Name: Classif, Length: 932671, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 200, 510, 300, 400, 500, 600, 520, 530]\n"
     ]
    }
   ],
   "source": [
    "print(classes_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/eos/jeodpp/data/projects/REFOCUS/classification/RFmodel_LUCAS_[1]_level1-mask_all-polygons_janv-aug2018_24042020'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mOSError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9bcc110ce59a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'RFmodel_LUCAS_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiome\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_level1-mask_all-polygons_janv-aug2018_24042020'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/eos/jeodpp/data/projects/REFOCUS/classification/RFmodel_LUCAS_[1]_level1-mask_all-polygons_janv-aug2018_24042020'"
     ]
    }
   ],
   "source": [
    "pickle.dump(rfc, open(local+'RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-aug2018_24042020', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=0, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=4, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=40,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Feature ranking:\n",
      "1. feature 12 (0.081311) VH_20180501\n",
      "2. feature 13 (0.068887) VH_20180511\n",
      "3. feature 11 (0.064408) VH_20180421\n",
      "4. feature 22 (0.056444) VV_20180101\n",
      "5. feature 10 (0.051337) VH_20180411\n",
      "6. feature 21 (0.049486) VH_20180730\n",
      "7. feature 14 (0.044355) VH_20180521\n",
      "8. feature 9 (0.042007) VH_20180401\n",
      "9. feature 41 (0.029622) VV_20180710\n",
      "10. feature 20 (0.028934) VH_20180720\n",
      "11. feature 5 (0.028927) VH_20180220\n",
      "12. feature 34 (0.028375) VV_20180501\n",
      "13. feature 43 (0.027530) VV_20180730\n",
      "14. feature 19 (0.024861) VH_20180710\n",
      "15. feature 15 (0.021888) VH_20180531\n",
      "16. feature 24 (0.021348) VV_20180121\n",
      "17. feature 35 (0.020903) VV_20180511\n",
      "18. feature 33 (0.020728) VV_20180421\n",
      "19. feature 6 (0.020477) VH_20180302\n",
      "20. feature 8 (0.019869) VH_20180322\n",
      "21. feature 25 (0.018263) VV_20180131\n",
      "22. feature 42 (0.017373) VV_20180720\n",
      "23. feature 30 (0.016480) VV_20180322\n",
      "24. feature 23 (0.014407) VV_20180111\n",
      "25. feature 36 (0.013514) VV_20180521\n",
      "26. feature 39 (0.013137) VV_20180620\n",
      "27. feature 29 (0.013015) VV_20180312\n",
      "28. feature 7 (0.011887) VH_20180312\n",
      "29. feature 17 (0.011276) VH_20180620\n",
      "30. feature 18 (0.011169) VH_20180630\n",
      "31. feature 40 (0.011046) VV_20180630\n",
      "32. feature 2 (0.011000) VH_20180121\n",
      "33. feature 16 (0.009475) VH_20180610\n",
      "34. feature 28 (0.009099) VV_20180302\n",
      "35. feature 4 (0.008299) VH_20180210\n",
      "36. feature 38 (0.008079) VV_20180610\n",
      "37. feature 26 (0.008050) VV_20180210\n",
      "38. feature 37 (0.007381) VV_20180531\n",
      "39. feature 31 (0.007088) VV_20180401\n",
      "40. feature 0 (0.006685) VH_20180101\n",
      "41. feature 32 (0.006609) VV_20180411\n",
      "42. feature 1 (0.005238) VH_20180111\n",
      "43. feature 3 (0.005101) VH_20180131\n",
      "44. feature 27 (0.004632) VV_20180220\n"
     ]
    }
   ],
   "source": [
    "Model=rfc\n",
    "print (Model)\n",
    "\n",
    "importances = Model.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in Model], axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(44):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]),X_train.columns[indices[f]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
      "          estimator=Pipeline(memory=None,\n",
      "     steps=[('Scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('RFclf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "   ...obs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))]),\n",
      "          fit_params=None, iid='warn', n_iter=100, n_jobs=38,\n",
      "          param_distributions={'RFclf__min_samples_leaf': [1, 2, 3, 4, 8, 10, 12], 'RFclf__max_features': ['auto', 'sqrt', 'log2'], 'RFclf__bootstrap': [0, 1], 'RFclf__max_depth': [4, 5, 6, 8, 12, None], 'RFclf__criterion': ['gini', 'entropy'], 'RFclf__min_samples_split': [3, 5, 7], 'RFclf__n_estimators': [50, 75, 100, 130, 150, 180]},\n",
      "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
      "          return_train_score='warn', scoring=None, verbose=1)\n",
      "Feature ranking:\n",
      "1. feature 10 (0.075501) VH_20180411\n",
      "2. feature 11 (0.067942) VH_20180421\n",
      "3. feature 12 (0.062540) VH_20180501\n",
      "4. feature 6 (0.056450) VH_20180302\n",
      "5. feature 2 (0.044835) VH_20180121\n",
      "6. feature 5 (0.039275) VH_20180220\n",
      "7. feature 9 (0.035977) VH_20180401\n",
      "8. feature 33 (0.033210) VV_20180421\n",
      "9. feature 21 (0.031234) VH_20180730\n",
      "10. feature 34 (0.027534) VV_20180501\n",
      "11. feature 22 (0.024591) VV_20180101\n",
      "12. feature 18 (0.024174) VH_20180630\n",
      "13. feature 29 (0.023097) VV_20180312\n",
      "14. feature 16 (0.022334) VH_20180610\n",
      "15. feature 19 (0.021676) VH_20180710\n",
      "16. feature 13 (0.021527) VH_20180511\n",
      "17. feature 14 (0.020505) VH_20180521\n",
      "18. feature 15 (0.019968) VH_20180531\n",
      "19. feature 20 (0.019392) VH_20180720\n",
      "20. feature 4 (0.018986) VH_20180210\n",
      "21. feature 1 (0.017203) VH_20180111\n",
      "22. feature 0 (0.017197) VH_20180101\n",
      "23. feature 30 (0.017070) VV_20180322\n",
      "24. feature 31 (0.016648) VV_20180401\n",
      "25. feature 17 (0.015948) VH_20180620\n",
      "26. feature 35 (0.015863) VV_20180511\n",
      "27. feature 28 (0.015859) VV_20180302\n",
      "28. feature 36 (0.015513) VV_20180521\n",
      "29. feature 3 (0.014881) VH_20180131\n",
      "30. feature 8 (0.014796) VH_20180322\n",
      "31. feature 23 (0.014752) VV_20180111\n",
      "32. feature 25 (0.014153) VV_20180131\n",
      "33. feature 41 (0.012528) VV_20180710\n",
      "34. feature 37 (0.011781) VV_20180531\n",
      "35. feature 43 (0.010961) VV_20180730\n",
      "36. feature 24 (0.010939) VV_20180121\n",
      "37. feature 26 (0.010151) VV_20180210\n",
      "38. feature 32 (0.009973) VV_20180411\n",
      "39. feature 7 (0.009604) VH_20180312\n",
      "40. feature 42 (0.009513) VV_20180720\n",
      "41. feature 27 (0.009033) VV_20180220\n",
      "42. feature 39 (0.008440) VV_20180620\n",
      "43. feature 40 (0.008305) VV_20180630\n",
      "44. feature 38 (0.008143) VV_20180610\n"
     ]
    }
   ],
   "source": [
    "# Features importances for the best RF model from the Grid CV\n",
    "Model=rfc\n",
    "print(Model)\n",
    "importances = Model.best_estimator_.steps[1][1].feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in Model.best_estimator_.steps[1][1]], axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(44):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]),X_train.columns[indices[f]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST FOR ONE SET OF PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=0, class_weight=None, criterion='entropy',\n",
      "            max_depth=12, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=4, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Random Forest\n",
      "Elapsed time for training: 41.23 sec\n"
     ]
    }
   ],
   "source": [
    "#TEST WITH RANDOM PARAMETERS TO TEST FEATURES IMPORTANCE\n",
    "#July\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(bootstrap=0, criterion='entropy', max_depth=12, max_features='sqrt', \n",
    "                             min_samples_leaf=4, min_samples_split=3, n_estimators=150, n_jobs=40)\n",
    "\n",
    "rfc.fit(X_train.values, y_train.values)\n",
    "\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "#pickle.dump(clf, open('RFmodel_1x1_scenario2_test', 'wb'))\n",
    "\n",
    "import pickle\n",
    "pickle.dump(rfc, open('RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-jul2018_24042020', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=0, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=4, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=40,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Feature ranking:\n",
      "1. feature 0 (0.074120) VH_20180101\n",
      "2. feature 2 (0.073029) VH_20180121\n",
      "3. feature 20 (0.052349) VH_20180720\n",
      "4. feature 21 (0.049794) VH_20180730\n",
      "5. feature 1 (0.042506) VH_20180111\n",
      "6. feature 19 (0.040007) VH_20180710\n",
      "7. feature 3 (0.038395) VH_20180131\n",
      "8. feature 5 (0.031169) VH_20180220\n",
      "9. feature 33 (0.030943) VV_20180421\n",
      "10. feature 4 (0.028349) VH_20180210\n",
      "11. feature 18 (0.027240) VH_20180630\n",
      "12. feature 7 (0.024927) VH_20180312\n",
      "13. feature 11 (0.024236) VH_20180421\n",
      "14. feature 9 (0.023860) VH_20180401\n",
      "15. feature 8 (0.023179) VH_20180322\n",
      "16. feature 10 (0.021488) VH_20180411\n",
      "17. feature 6 (0.021442) VH_20180302\n",
      "18. feature 32 (0.021435) VV_20180411\n",
      "19. feature 26 (0.016902) VV_20180210\n",
      "20. feature 12 (0.016560) VH_20180501\n",
      "21. feature 42 (0.016284) VV_20180720\n",
      "22. feature 40 (0.015825) VV_20180630\n",
      "23. feature 43 (0.015811) VV_20180730\n",
      "24. feature 17 (0.015772) VH_20180620\n",
      "25. feature 16 (0.015104) VH_20180610\n",
      "26. feature 15 (0.014916) VH_20180531\n",
      "27. feature 14 (0.014499) VH_20180521\n",
      "28. feature 39 (0.013952) VV_20180620\n",
      "29. feature 27 (0.013750) VV_20180220\n",
      "30. feature 25 (0.013750) VV_20180131\n",
      "31. feature 41 (0.013521) VV_20180710\n",
      "32. feature 31 (0.013350) VV_20180401\n",
      "33. feature 13 (0.013236) VH_20180511\n",
      "34. feature 36 (0.013143) VV_20180521\n",
      "35. feature 24 (0.012669) VV_20180121\n",
      "36. feature 28 (0.012279) VV_20180302\n",
      "37. feature 34 (0.012092) VV_20180501\n",
      "38. feature 23 (0.011980) VV_20180111\n",
      "39. feature 22 (0.011866) VV_20180101\n",
      "40. feature 29 (0.011185) VV_20180312\n",
      "41. feature 37 (0.010983) VV_20180531\n",
      "42. feature 30 (0.010883) VV_20180322\n",
      "43. feature 38 (0.010682) VV_20180610\n",
      "44. feature 35 (0.010538) VV_20180511\n"
     ]
    }
   ],
   "source": [
    "Model=rfc\n",
    "print (Model)\n",
    "\n",
    "importances = Model.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in Model], axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(44):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]),X_train.columns[indices[f]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'grid_scores_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-172657d620eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'grid_scores_'"
     ]
    }
   ],
   "source": [
    "rfc.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all the models tried\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to import in GEE\n",
    "#index_column_VHVV=lucas.columns[76:148]\n",
    "#index_column_class=lucas.columns[1:4]\n",
    "#print(index_column_class)\n",
    "#print(index_column_VHVV)\n",
    "#create a csv with all the data\n",
    "#lucas_gee=lucas.loc[:,index_column_class.union(index_column_VHVV)]\n",
    "#lucas_gee.to_csv('lucas_crop_grass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform in byte\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler\n",
    "#data=X_features\n",
    "\n",
    "#scaler.data_max_=0\n",
    "#scaler.data_min_=-25\n",
    "\n",
    "#print(scaler.data_max_)\n",
    "#scaler.fit(lucas_test)\n",
    "#scaler.fit_transform(lucas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some test to avoid loading polygons csv\n",
    "#goupby by class and polygon - give count of pixels per polygons\n",
    "#test=pd_level2.groupby(['ClassL2','POINT_I'])['ClassL2','POINT_I'].count()\n",
    "#print(test.head())\n",
    "#how to count the number of polygon per class? level 0?\n",
    "#lucasgroup=pd_level2.groupby(['ClassL2','POINT_I'])['POINT_I'].size()\n",
    "#print(lucasgroup.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement with the biome class for the main crops we are interested\n",
    "#+the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
