{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUCAS COPERNICUS\n",
    "# Creation of the random forest models for the Eurocropmap\n",
    "## crop types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JEODPP\n",
    "data_path='/eos/jeodpp/data/projects/REFOCUS/data/S1_GS/all-10days/Map_v7/'\n",
    "project_path='/eos/jeodpp/data/projects/REFOCUS/classification/'\n",
    "path_pol = '/eos/jeodpp/data/projects/REFOCUS/data/polygons/v7'\n",
    "results='/eos/jeodpp/data/projects/REFOCUS/classification/'\n",
    "\n",
    "local='/eos/jeodpp/home/users/verheas/data/LUCAS/v7/'\n",
    "\n",
    "# RAF LOCAL\n",
    "# data_path='/data/LUCAS-cop-single-pixel'\n",
    "# project_path='/data/Dropbox/JRC/LANDSENSE/CASE-STUDY-8-LUCAS-COPERNICUS-CLASSIF/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "#import geopandas as gdp\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#pd.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################Parameters##################################################\n",
    "#classes - stored in a table 'legend-lucas-all'\n",
    "table_class=pd.read_csv(os.path.join(project_path,'table/legend-lucas-all-v7.csv'),dtype=pd.Int64Dtype())\n",
    "\n",
    "classes_L1=list(table_class['classes_L1'].dropna()) \n",
    "classes_L2=list(table_class['classes_L2'].dropna())\n",
    "\n",
    "#Biome selection\n",
    "\n",
    "biome_1=[1]\n",
    "biome_2=[2]\n",
    "biome_3=[3]\n",
    "biome_4=[4]\n",
    "\n",
    "#level\n",
    "level_1='level_1'\n",
    "level_2='level_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd_lucas (2956889, 116)\n",
      "300    1216530\n",
      "200    1000318\n",
      "500     732964\n",
      "600       3856\n",
      "100       3221\n",
      "Name: level_1, dtype: int64\n",
      "300    1216530\n",
      "500     732964\n",
      "211     290116\n",
      "213     142886\n",
      "216     125644\n",
      "232      64452\n",
      "290      63609\n",
      "250      59053\n",
      "214      35215\n",
      "231      34577\n",
      "240      34067\n",
      "215      34005\n",
      "212      28825\n",
      "222      23174\n",
      "218      19274\n",
      "221      15815\n",
      "230      12000\n",
      "233       7094\n",
      "223       4920\n",
      "219       4724\n",
      "600       3856\n",
      "100       3221\n",
      "217        868\n",
      "Name: level_2, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (27,30,33,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>POINT_ID</th>\n",
       "      <th>NUTS0</th>\n",
       "      <th>NUTS1</th>\n",
       "      <th>NUTS2</th>\n",
       "      <th>NUTS3</th>\n",
       "      <th>TH_LAT</th>\n",
       "      <th>TH_LONG</th>\n",
       "      <th>OFFICE_PI</th>\n",
       "      <th>EX_ANTE</th>\n",
       "      <th>...</th>\n",
       "      <th>LU2_LABEL</th>\n",
       "      <th>LU1_TYPE_LABEL</th>\n",
       "      <th>LU2_TYPE_LABEL</th>\n",
       "      <th>CPRN_LC_LABEL</th>\n",
       "      <th>CPRN_LC_SAME_LC1</th>\n",
       "      <th>LUCAS_CORE_INTERSECT</th>\n",
       "      <th>COPERNICUS_CLEANED</th>\n",
       "      <th>stratum</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34562080</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES2</td>\n",
       "      <td>ES24</td>\n",
       "      <td>ES243</td>\n",
       "      <td>41.288386</td>\n",
       "      <td>-0.319428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other bare soil</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>290</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31243520</td>\n",
       "      <td>IE</td>\n",
       "      <td>IE0</td>\n",
       "      <td>IE04</td>\n",
       "      <td>IE042</td>\n",
       "      <td>53.422977</td>\n",
       "      <td>-8.226052</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spontaneously re-vegetated surfaces</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>33661774</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES5</td>\n",
       "      <td>ES52</td>\n",
       "      <td>ES521</td>\n",
       "      <td>38.434388</td>\n",
       "      <td>-0.905705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Permanent crops: fruit trees</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>28922250</td>\n",
       "      <td>ES</td>\n",
       "      <td>ES1</td>\n",
       "      <td>ES11</td>\n",
       "      <td>ES113</td>\n",
       "      <td>41.867145</td>\n",
       "      <td>-7.320304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shrubland with sparse tree cover</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>35082906</td>\n",
       "      <td>FR</td>\n",
       "      <td>FRD</td>\n",
       "      <td>FRD1</td>\n",
       "      <td>FRD12</td>\n",
       "      <td>48.715190</td>\n",
       "      <td>-1.092190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Not relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Grassland without tree/shrub cover</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  POINT_ID NUTS0 NUTS1 NUTS2  NUTS3     TH_LAT   TH_LONG  \\\n",
       "0           0  34562080    ES   ES2  ES24  ES243  41.288386 -0.319428   \n",
       "1           1  31243520    IE   IE0  IE04  IE042  53.422977 -8.226052   \n",
       "2           2  33661774    ES   ES5  ES52  ES521  38.434388 -0.905705   \n",
       "3           3  28922250    ES   ES1  ES11  ES113  41.867145 -7.320304   \n",
       "4           4  35082906    FR   FRD  FRD1  FRD12  48.715190 -1.092190   \n",
       "\n",
       "   OFFICE_PI  EX_ANTE  ...     LU2_LABEL  LU1_TYPE_LABEL  LU2_TYPE_LABEL  \\\n",
       "0          0        0  ...  Not relevant             NaN             NaN   \n",
       "1          0        0  ...  Not relevant             NaN             NaN   \n",
       "2          0        0  ...  Not relevant             NaN             NaN   \n",
       "3          0        0  ...  Not relevant             NaN             NaN   \n",
       "4          0        0  ...  Not relevant             NaN             NaN   \n",
       "\n",
       "                         CPRN_LC_LABEL  CPRN_LC_SAME_LC1  \\\n",
       "0                      Other bare soil              True   \n",
       "1  Spontaneously re-vegetated surfaces              True   \n",
       "2         Permanent crops: fruit trees              True   \n",
       "3     Shrubland with sparse tree cover              True   \n",
       "4   Grassland without tree/shrub cover              True   \n",
       "\n",
       "   LUCAS_CORE_INTERSECT  COPERNICUS_CLEANED  stratum  level_2  level_1  \n",
       "0                  True                True        2      290      200  \n",
       "1                  True                True        1      500      500  \n",
       "2                  True                True        2      300      300  \n",
       "3                  True                True        2      300      300  \n",
       "4                  True                True        1      500      500  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the data\n",
    "#1) load the S1 10 days extracted values in GEE for all polygons\n",
    "\n",
    "pd_lucas= pd.read_csv(os.path.join(data_path,'S1_point_allV7_10days_10m_1Jan-31Dec_EU_ratio-db.csv'),dtype={'level_1':int,'level_2':int})\n",
    "print('pd_lucas',pd_lucas.shape)\n",
    "\n",
    "#concatenate all the data in one dataframe\n",
    "#group cropland, grassland and bareland \n",
    "#number of pixels per class\n",
    "print(pd_lucas.level_1.value_counts())\n",
    "print(pd_lucas.level_2.value_counts())\n",
    "pd_lucas.head()\n",
    "\n",
    "#number of pixels per class\n",
    "#pd_lucas.LC1_COD.value_counts()\n",
    "#pd_lucas.head()\n",
    "pd_lucas.columns\n",
    "\n",
    "##############1.2 Load the shapefile with the polygons - useful to split the polygons in training and test dataset for the accuracy ######################\n",
    "# load csv with of the polygons\n",
    "#2)load csv with the polygons for the split test/validation\n",
    "lucas_polygons = pd.read_csv(os.path.join(path_pol,'LUCAS_2018_Copernicus_attributes_cropmap_level1-2.csv'))\n",
    "lucas_polygons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('biome',pd_lucas.stratum.value_counts())\n",
    "#print(pd_lucas.groupby('POINT_ID').apply(min).shape)\n",
    "#print(pd_lucas.groupby('POINT_ID').apply(min).stratum.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biome 1 - parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biome [1]\n",
      "level level_2\n",
      "level [211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 230, 231, 232, 233, 240, 250, 290]\n"
     ]
    }
   ],
   "source": [
    "###################################Choose parameters for this run #############################################\n",
    "#Biome\n",
    "biome=biome_1\n",
    "print('biome',biome)\n",
    "\n",
    "#level\n",
    "level=level_2\n",
    "print('level',level)\n",
    "#crop - level 2, from the table we load only the crop type classes\n",
    "classes=classes_L2\n",
    "print('level',classes)\n",
    "\n",
    "#Split for the train/test dataset - we run it with all the polygons\n",
    "#split_test = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome 1 - Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2956889, 117)\n",
      "pd_lucas_b 211    261290\n",
      "216    119697\n",
      "213     98670\n",
      "232     63899\n",
      "250     40453\n",
      "290     35167\n",
      "214     31956\n",
      "215     25021\n",
      "231     24369\n",
      "222     22706\n",
      "240     22440\n",
      "218     17686\n",
      "221     14965\n",
      "212     10863\n",
      "230      7600\n",
      "233      7020\n",
      "219      4625\n",
      "223      4036\n",
      "217       661\n",
      "Name: Classif, dtype: int64\n",
      "pd_lucas_b 300    1216530\n",
      "500     732964\n",
      "211     290116\n",
      "213     142886\n",
      "216     125644\n",
      "232      64452\n",
      "290      63609\n",
      "250      59053\n",
      "214      35215\n",
      "231      34577\n",
      "240      34067\n",
      "215      34005\n",
      "212      28825\n",
      "222      23174\n",
      "218      19274\n",
      "221      15815\n",
      "230      12000\n",
      "233       7094\n",
      "223       4920\n",
      "219       4724\n",
      "600       3856\n",
      "100       3221\n",
      "217        868\n",
      "Name: Classif, dtype: int64\n",
      "1    14138\n",
      "Name: stratum, dtype: int64\n",
      "X_train head       VH_20180101  VH_20180111  VH_20180121  VH_20180131  VH_20180210  \\\n",
      "3221   -19.922432   -19.665386   -21.799988   -21.300838   -21.750020   \n",
      "3222   -19.120277   -18.656569   -21.332876   -20.977938   -21.134995   \n",
      "3223   -19.106567   -18.033580   -20.940510   -21.219793   -21.626299   \n",
      "3224   -18.917624   -19.117985   -20.075660   -20.957410   -20.808330   \n",
      "3225   -20.198582   -19.372526   -21.501340   -21.408241   -21.743530   \n",
      "\n",
      "      VH_20180220  VH_20180302  VH_20180312  VH_20180322  VH_20180401  ...  \\\n",
      "3221   -22.062218   -21.734564   -21.260138   -22.881704   -20.128270  ...   \n",
      "3222   -20.734947   -21.102257   -22.271767   -22.423730   -20.382513  ...   \n",
      "3223   -20.367079   -20.484667   -20.963932   -22.211182   -20.511623  ...   \n",
      "3224   -21.128971   -21.083132   -21.680656   -21.603765   -19.775436  ...   \n",
      "3225   -20.840073   -21.085110   -21.452280   -22.337664   -20.231638  ...   \n",
      "\n",
      "      VV_20180501  VV_20180511  VV_20180521  VV_20180531  VV_20180610  \\\n",
      "3221   -15.712803   -13.505243   -14.567000   -11.906336   -11.947569   \n",
      "3222   -15.673021   -13.820341   -15.170387   -12.954488   -11.727719   \n",
      "3223   -15.047111   -13.692605   -14.219698   -13.187538   -11.330023   \n",
      "3224   -16.541920   -12.420581   -12.764953   -12.475316   -11.373104   \n",
      "3225   -16.262772   -13.600579   -13.661297   -12.707240   -11.357501   \n",
      "\n",
      "      VV_20180620  VV_20180630  VV_20180710  VV_20180720  VV_20180730  \n",
      "3221   -12.103310   -12.104501   -10.881879    -7.880724    -8.418904  \n",
      "3222   -12.220854   -11.392782   -10.940816    -7.537317    -9.175847  \n",
      "3223   -12.490657   -10.731218    -9.853989    -7.828915    -9.365373  \n",
      "3224   -10.847144   -11.218471   -11.202961    -7.668771    -9.804807  \n",
      "3225   -12.367601   -11.306118   -11.646503    -7.432031    -9.885860  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "X_train shape (813124, 44)\n",
      "y_train shape (813124,)\n",
      "y_train count 211    261290\n",
      "216    119697\n",
      "213     98670\n",
      "232     63899\n",
      "250     40453\n",
      "290     35167\n",
      "214     31956\n",
      "215     25021\n",
      "231     24369\n",
      "222     22706\n",
      "240     22440\n",
      "218     17686\n",
      "221     14965\n",
      "212     10863\n",
      "230      7600\n",
      "233      7020\n",
      "219      4625\n",
      "223      4036\n",
      "217       661\n",
      "Name: Classif, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#### 2) Prepare the data for the classification ##############\n",
    "##############################################################\n",
    "\n",
    "#############2.1 Select level of work and classes\n",
    "#copy values in a new column 'Classif' that we will use in the rest of the script\n",
    "pd_lucas['Classif']=pd_lucas[level]\n",
    "print(pd_lucas.shape)\n",
    "\n",
    "pd_lucas_i=pd_lucas[pd_lucas.Classif.isin(classes_L2)]\n",
    "\n",
    "#############2.2 Select the biome\n",
    "#select biome\n",
    "pd_lucas_b=pd_lucas_i[pd_lucas_i.stratum.isin(biome)]\n",
    "print('pd_lucas_b',pd_lucas_b.Classif.value_counts())\n",
    "print('pd_lucas_b',pd_lucas.Classif.value_counts())\n",
    "print(pd_lucas_b.groupby('POINT_ID').apply(min).stratum.value_counts())\n",
    "\n",
    "#############2.3 Select the data inputs for the classification\n",
    "## we use all the polygons, therefore there are no test dataset\n",
    "\n",
    "X_train=pd_lucas_b.filter(regex='(((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-7])')\n",
    "y_train=pd_lucas_b['Classif']\n",
    "\n",
    "print('X_train head',X_train.head())\n",
    "print('X_train shape',X_train.shape)\n",
    "\n",
    "print('y_train shape',y_train.shape)\n",
    "print('y_train count',y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biome 1    813124\n",
      "Name: stratum, dtype: int64\n",
      "1    14138\n",
      "Name: stratum, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('biome',pd_lucas_b.stratum.value_counts())\n",
    "#print(pd_lucas.groupby('POINT_ID').apply(sum).shape)\n",
    "print(pd_lucas_b.groupby('POINT_ID').apply(min).stratum.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(bootstrap=True,criterion='gini', max_depth=None, max_features='sqrt', \n",
    "                             n_estimators=1200, n_jobs=40,min_samples_split=2,min_samples_leaf=1)\n",
    "rfc.fit(X_train.values, y_train.values)\n",
    "\n",
    "pickle.dump(rfc, open(os.path.join('RFmodel_LUCAS_'+str(biome)+'_'+str(level)+'_all-polygons_janv-jul2018_15122020bdesk'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome 1 - Run a RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to many samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search for a set of parameters - without scaler - no need of pipeline?\n",
    "#there is also the randomized search CV that will not run all the possibilities - we can use to get a first idea\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = { \n",
    "    'RFclf__n_estimators': [300,500,800,900,1000,1100,1200], #The number of trees in the forest.\n",
    "    'RFclf__max_features': ['sqrt', 'log2'], #The number of features to consider when looking for the best split:\n",
    "    'RFclf__max_depth': [5,10,15,25,50,None],#The maximum depth of the tree\n",
    "    'RFclf__min_samples_leaf': [1,2,3,4,8,10,12],#The minimum number of samples in newly created leaves.\n",
    "    #'RFclf__bootstrap': [0, 1],#Whether bootstrap samples are used when building trees.\n",
    "    'RFclf__min_samples_split': [1,10,25,50,70], #The minimum number of samples required to split an internal node\n",
    "    'RFclf__criterion': ['gini', 'entropy']#The function to measure the quality of a split\n",
    "}\n",
    "#Scaler = preprocessing.StandardScaler()\n",
    "#Scaler.fit(TrF)\n",
    "Pipeline = Pipeline([('RFclf', RandomForestClassifier())])\n",
    "#rfc = GridSearchCV(estimator=Pipeline, param_grid=param_grid,cv=3, n_jobs=100, verbose=1)\n",
    "rfc = RandomizedSearchCV(estimator=Pipeline, param_distributions =param_grid, n_iter=42,cv=3, n_jobs=42, verbose=1)\n",
    "#RandomizedSearchCV\n",
    "#rfc.fit(Scaler.transform(TrF), TrC)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "\n",
    "\n",
    "pickle.dump(rfc, open(os.path.join(project_path,'RFmodel_LUCAS_'+str(biome)+'_'+str(level)+'_all-polygons_janv-jul2018_01012020best2'), 'wb'))\n",
    "\n",
    "print(rfc.best_estimator_.named_steps['RFclf'])\n",
    "\n",
    "#best model\n",
    "#print('best estimator',best_model)\n",
    "\n",
    "#pickle.dump(best_model, open('RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-jul2018_160920-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Random Forest\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/verheas/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 28 is smaller than n_iter=42. Running 28 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "#Grid Search for a set of parameters - without scaler - no need of pipeline?\n",
    "#there is also the randomized search CV that will not run all the possibilities - we can use to get a first idea\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = { \n",
    "    'RFclf__n_estimators': [300,500,800,900,1000,1100,1200], #The number of trees in the forest.\n",
    "    'RFclf__max_features': ['sqrt', 'log2'], #The number of features to consider when looking for the best split:\n",
    "    #'RFclf__max_depth': [5,10,15,25,50,None],#The maximum depth of the tree\n",
    "    #'RFclf__min_samples_leaf': [1,2,3,4,8,10,12],#The minimum number of samples in newly created leaves.\n",
    "    #'RFclf__bootstrap': [0, 1],#Whether bootstrap samples are used when building trees.\n",
    "    #'RFclf__min_samples_split': [1,10,25,50,70], #The minimum number of samples required to split an internal node\n",
    "    'RFclf__criterion': ['gini', 'entropy']#The function to measure the quality of a split\n",
    "}\n",
    "#Scaler = preprocessing.StandardScaler()\n",
    "#Scaler.fit(TrF)\n",
    "Pipeline = Pipeline([('RFclf', RandomForestClassifier())])\n",
    "#rfc = GridSearchCV(estimator=Pipeline, param_grid=param_grid,cv=3, n_jobs=100, verbose=1)\n",
    "rfc = RandomizedSearchCV(estimator=Pipeline, param_distributions =param_grid, n_iter=42,cv=3, verbose=1)\n",
    "#RandomizedSearchCV\n",
    "#rfc.fit(Scaler.transform(TrF), TrC)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "\n",
    "\n",
    "pickle.dump(rfc, open(os.path.join(project_path,'RFmodel_LUCAS_'+str(biome)+'_'+str(level)+'_all-polygons_janv-jul2018_15122020best-smallgrid'), 'wb'))\n",
    "\n",
    "print(rfc.best_estimator_.named_steps['RFclf'])\n",
    "\n",
    "#best model\n",
    "#print('best estimator',best_model)\n",
    "\n",
    "#pickle.dump(best_model, open('RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-jul2018_160920-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rfc, open(os.path.join('RFmodel_LUCAS_'+str(biome)+'_'+str(level)+'_all-polygons_janv-jul2018_15122020best-smallgridter'), 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biome 2 - choose parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biome [2]\n",
      "level level_2\n",
      "level [211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 230, 231, 232, 233, 240, 250, 290]\n"
     ]
    }
   ],
   "source": [
    "###################################Choose parameters for this run #############################################\n",
    "#Biome\n",
    "biome=biome_2\n",
    "print('biome',biome)\n",
    "\n",
    "#level\n",
    "level=level_2\n",
    "print('level',level)\n",
    "#crop - level 2, from the table we load only the crop type classes\n",
    "classes=classes_L2\n",
    "print('level',classes)\n",
    "\n",
    "#Split for the train/test dataset - we run it with all the polygons\n",
    "#split_test = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biome 2 - prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2956889, 117)\n",
      "pd_lucas_b 213    44216\n",
      "211    28826\n",
      "290    28442\n",
      "250    18600\n",
      "212    17962\n",
      "240    11627\n",
      "231    10208\n",
      "215     8984\n",
      "216     5947\n",
      "230     4400\n",
      "214     3259\n",
      "218     1588\n",
      "223      884\n",
      "221      850\n",
      "232      553\n",
      "222      468\n",
      "217      207\n",
      "219       99\n",
      "233       74\n",
      "Name: Classif, dtype: int64\n",
      "pd_lucas_b 300    1216530\n",
      "500     732964\n",
      "211     290116\n",
      "213     142886\n",
      "216     125644\n",
      "232      64452\n",
      "290      63609\n",
      "250      59053\n",
      "214      35215\n",
      "231      34577\n",
      "240      34067\n",
      "215      34005\n",
      "212      28825\n",
      "222      23174\n",
      "218      19274\n",
      "221      15815\n",
      "230      12000\n",
      "233       7094\n",
      "223       4920\n",
      "219       4724\n",
      "600       3856\n",
      "100       3221\n",
      "217        868\n",
      "Name: Classif, dtype: int64\n",
      "2    4233\n",
      "Name: stratum, dtype: int64\n",
      "X_train head        VH_20180101  VH_20180111  VH_20180121  VH_20180131  VH_20180210  \\\n",
      "89408   -17.420542   -17.807598   -18.702505   -18.363735   -17.056997   \n",
      "89409   -18.150043   -17.900179   -19.867800   -18.586760   -17.160088   \n",
      "89410   -15.249046   -16.180910   -17.672636   -18.044584   -16.584139   \n",
      "89411   -16.527863   -16.714607   -18.103354   -18.798822   -18.433475   \n",
      "89412   -17.348940   -16.499746   -19.760744   -19.186266   -18.050493   \n",
      "\n",
      "       VH_20180220  VH_20180302  VH_20180312  VH_20180322  VH_20180401  ...  \\\n",
      "89408   -17.429663   -16.462894   -16.374443   -16.596394   -16.646519  ...   \n",
      "89409   -17.442318   -18.565237   -17.200493   -15.501730   -17.202303  ...   \n",
      "89410   -17.734936   -14.750084   -15.017361   -14.171321   -16.405302  ...   \n",
      "89411   -17.617600   -16.919018   -17.018095   -15.866068   -17.723814  ...   \n",
      "89412   -16.410456   -17.906069   -17.436472   -15.864169   -17.496931  ...   \n",
      "\n",
      "       VV_20180501  VV_20180511  VV_20180521  VV_20180531  VV_20180610  \\\n",
      "89408   -14.881550   -13.101749   -10.444297   -11.692823   -10.200985   \n",
      "89409   -16.245447   -13.902846   -10.623051   -14.427812   -10.656303   \n",
      "89410   -12.470223   -10.368835    -8.299841    -9.987116    -9.612868   \n",
      "89411   -14.508311   -12.695195   -10.814119   -12.495983   -11.513090   \n",
      "89412   -15.111621   -14.044498   -10.721618   -15.181081   -11.564595   \n",
      "\n",
      "       VV_20180620  VV_20180630  VV_20180710  VV_20180720  VV_20180730  \n",
      "89408   -12.601353   -11.624434   -11.263647   -10.562478   -11.831532  \n",
      "89409   -14.128812   -11.817793   -13.582592   -12.652672   -12.363650  \n",
      "89410    -9.535673   -10.635033    -9.581246    -9.506692   -11.063625  \n",
      "89411   -12.858275   -11.327235   -11.775384   -11.764240   -11.562636  \n",
      "89412   -14.525645   -11.622237   -13.753021   -12.943723   -12.187632  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "X_train shape (187194, 44)\n",
      "y_train shape (187194,)\n",
      "y_train count 213    44216\n",
      "211    28826\n",
      "290    28442\n",
      "250    18600\n",
      "212    17962\n",
      "240    11627\n",
      "231    10208\n",
      "215     8984\n",
      "216     5947\n",
      "230     4400\n",
      "214     3259\n",
      "218     1588\n",
      "223      884\n",
      "221      850\n",
      "232      553\n",
      "222      468\n",
      "217      207\n",
      "219       99\n",
      "233       74\n",
      "Name: Classif, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#### 2) Prepare the data for the classification ##############\n",
    "##############################################################\n",
    "\n",
    "#############2.1 Select level of work and classes\n",
    "#copy values in a new column 'Classif' that we will use in the rest of the script\n",
    "pd_lucas['Classif']=pd_lucas[level]\n",
    "print(pd_lucas.shape)\n",
    "\n",
    "pd_lucas_i=pd_lucas[pd_lucas.Classif.isin(classes_L2)]\n",
    "\n",
    "#############2.2 Select the biome\n",
    "#select biome\n",
    "pd_lucas_b=pd_lucas_i[pd_lucas_i.stratum.isin(biome)]\n",
    "print('pd_lucas_b',pd_lucas_b.Classif.value_counts())\n",
    "print('pd_lucas_b',pd_lucas.Classif.value_counts())\n",
    "print(pd_lucas_b.groupby('POINT_ID').apply(min).stratum.value_counts())\n",
    "\n",
    "#############2.3 Select the data inputs for the classification\n",
    "## we use all the polygons, therefore there are no test dataset\n",
    "\n",
    "X_train=pd_lucas_b.filter(regex='(((?<![\\w\\d])VH_)|((?<![\\w\\d])VV_))(20180[1-7])')\n",
    "y_train=pd_lucas_b['Classif']\n",
    "\n",
    "print('X_train head',X_train.head())\n",
    "print('X_train shape',X_train.shape)\n",
    "\n",
    "print('y_train shape',y_train.shape)\n",
    "print('y_train count',y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a RandomizedSearchCV for Biome 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Random Forest\n",
      "Fitting 3 folds for each of 18144 candidates, totalling 54432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=100)]: Using backend LokyBackend with 100 concurrent workers.\n",
      "/home/verheas/.local/lib/python3.5/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=100)]: Done 250 tasks      | elapsed: 131.2min\n",
      "[Parallel(n_jobs=100)]: Done 600 tasks      | elapsed: 299.6min\n",
      "[Parallel(n_jobs=100)]: Done 1050 tasks      | elapsed: 522.6min\n"
     ]
    }
   ],
   "source": [
    "#Grid Search for a set of parameters - without scaler - no need of pipeline?\n",
    "#there is also the randomized search CV that will not run all the possibilities - we can use to get a first idea\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = { \n",
    "    'RFclf__n_estimators': [150,180,210,240,270,300,500,800,900,1000,1100,1200], #The number of trees in the forest.\n",
    "    'RFclf__max_features': ['auto', 'sqrt', 'log2'], #The number of features to consider when looking for the best split:\n",
    "    'RFclf__max_depth': [4,5,6,8,12,None],#The maximum depth of the tree\n",
    "    'RFclf__min_samples_leaf': [1,2,3,4,8,10,12],#The minimum number of samples in newly created leaves.\n",
    "    'RFclf__bootstrap': [0, 1],#Whether bootstrap samples are used when building trees.\n",
    "    'RFclf__min_samples_split': [3, 5, 7], #The minimum number of samples required to split an internal node\n",
    "    'RFclf__criterion': ['gini', 'entropy']#The function to measure the quality of a split\n",
    "}\n",
    "\n",
    "#Scaler = preprocessing.StandardScaler()\n",
    "#Scaler.fit(TrF)\n",
    "Pipeline = Pipeline([('RFclf', RandomForestClassifier())])\n",
    "rfc = GridSearchCV(estimator=Pipeline, param_grid=param_grid,cv=3, n_jobs=100, verbose=1)\n",
    "#rfc = BayesSearchCV(estimator=Pipeline, param_distributions =param_grid, n_iter=100,cv=3, n_jobs=50, verbose=1)\n",
    "#rfc = RandomizedSearchCV(estimator=Pipeline, param_distributions =param_grid, n_iter=100,cv=3, n_jobs=38, verbose=1)\n",
    "\n",
    "#RandomizedSearchCV\n",
    "#rfc.fit(Scaler.transform(TrF), TrC)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "\n",
    "\n",
    "pickle.dump(rfc, open(os.path.join(project_path,'RFmodel_LUCAS_'+str(biome)+'_'+str(level)+'_all-polygons_janv-jul2018_15122020best-smallgrid'), 'wb'))\n",
    "\n",
    "print(rfc.best_estimator_.named_steps['RFclf'])\n",
    "\n",
    "#best model\n",
    "#print('best estimator',best_model)\n",
    "\n",
    "#pickle.dump(best_model, open('RFmodel_LUCAS_'+str(biome)+'_level1-mask_all-polygons_janv-jul2018_160920-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Random Forest\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=38)]: Using backend LokyBackend with 38 concurrent workers.\n",
      "[Parallel(n_jobs=38)]: Done 124 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=38)]: Done 300 out of 300 | elapsed: 29.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for training: 2263.73 sec\n",
      "best estimator RandomForestClassifier(bootstrap=1, class_weight=None, criterion='entropy',\n",
      "            max_depth=12, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=7,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "#Grid Search for a set of parameters - takes time \n",
    "#there is also the randomized search CV that will not run all the possibilities - we can use to get a first idea\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = { \n",
    "    'RFclf__n_estimators': [25,50,75,100,130,150],\n",
    "    'RFclf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'RFclf__max_depth': [4,5,6,8,12,None],\n",
    "    'RFclf__min_samples_leaf': [1,2,3,4,8,10,12],\n",
    "    'RFclf__bootstrap': [0, 1],\n",
    "    'RFclf__min_samples_split': [3, 5, 7],\n",
    "    'RFclf__criterion': ['gini', 'entropy']\n",
    "}\n",
    "#Scaler = preprocessing.StandardScaler()\n",
    "#Scaler.fit(TrF)\n",
    "Pipeline = Pipeline([('Scaler', preprocessing.StandardScaler()), ('RFclf', RandomForestClassifier())])\n",
    "#rfc = GridSearchCV(estimator=Pipeline, param_grid=param_grid,cv=3, n_jobs=100, verbose=1)\n",
    "rfc = RandomizedSearchCV(estimator=Pipeline, param_distributions =param_grid, n_iter=100,cv=3, n_jobs=38, verbose=1)\n",
    "#RandomizedSearchCV\n",
    "#rfc.fit(Scaler.transform(TrF), TrC)\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "\n",
    "#best model\n",
    "#print('best estimator',rfc.best_estimator_.steps[1][1])\n",
    "#pickle.dump(rfc.best_estimator_.steps[1][1], open('RFmodel_LUCAS_'+str(biome)+'_level2-crop_all-polygons_janv-jul2018_24042020-best', 'wb'))\n",
    "#not correct?\n",
    "pickle.dump(rfc, open('RFmodel_LUCAS_'+str(biome)+'_level2-crop_all-polygons_janv-jul2018_160920-allmodels', 'wb'))\n",
    "\n",
    "best_model=rfc.best_estimator_.named_steps['RFclf']\n",
    "\n",
    "#best model\n",
    "print('best estimator',best_model)\n",
    "\n",
    "pickle.dump(best_model, open('RFmodel_LUCAS_'+str(biome)+'_level2-crop_all-polygons_janv-jul2018_160920-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=rfc.best_estimator_.steps[1][1]\n",
    "print('best estimator',best_model)\n",
    "\n",
    "pickle.dump(best_model, open('RFmodel_LUCAS_'+str(biome)+'_level2-crop_all-polygons_jan-jun2018_15092020-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best estimator RandomForestClassifier(bootstrap=0, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=4, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "#best model\n",
    "best_model=rfc.best_estimator_.steps[1][1]\n",
    "print('best estimator',best_model)\n",
    "\n",
    "pickle.dump(best_model, open('RFmodel_LUCAS_'+str(biome)+'_level2-crop_all-polygons_jan-jun2018_29062020-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=0, class_weight=None, criterion='entropy',\n",
      "            max_depth=12, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=4, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=40,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rfc.best_estimator_.steps[1][1], open('RFmodel_LUCAS_'+str(biome)+'_level2-crop_all-polygons_janv-jul2018_24042020-best', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features importances for the best RF model from the Grid CV\n",
    "Model=rfc\n",
    "\n",
    "importances = Model.best_estimator_.steps[1][1].feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in Model.best_estimator_.steps[1][1]], axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(44):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]),X_train.columns[indices[f]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Random Forest\n",
      "Elapsed time for training: 51.28 sec\n",
      "Accuracy is :86.86\n",
      "[[29965     0   197    61     6   495     0     0     0     1     6     0\n",
      "      0    34   106     0     6    34     4]\n",
      " [  665   312    34    30     0    20     0     0     0     0     0     0\n",
      "      0     1     1     0     1     0     1]\n",
      " [ 1472     0  9447    64     2   216     0     1     0     1     6     0\n",
      "      0     3    28     0     5    40     2]\n",
      " [  585     0   108  2733     8   167     0     0     0     1     1     0\n",
      "      0     6    18     0     1    17     0]\n",
      " [  558     0    94    53  1730    98     0     2     0     0     0     0\n",
      "      0     0     6     0     5    18     0]\n",
      " [  365     0    62    25     2 13174     0     0     0     0     6     0\n",
      "      0    61    36     2     0    17     0]\n",
      " [    6     0     0     0     0    16    54     0     0     0     0     0\n",
      "      0     4     0     0     0     6     0]\n",
      " [  703     0    66    97     2   102     0  1116     0     0     1     0\n",
      "      0     1     4     0     1    11     0]\n",
      " [   81     0     4     2     3   152     0     0   235     1     1     0\n",
      "      0    10     6     0     5     5     0]\n",
      " [    3     0     0     1     0   115     0     0     0  1502    47     0\n",
      "      0     7     4     0     5     0     0]\n",
      " [   80     0     2     0     0   169     0     0     0     4  2352     0\n",
      "      0    10     4     0     0     0     0]\n",
      " [   46     0     8     0     0    57     0     0     0     3     4   337\n",
      "      0     0     9     0     0     3     0]\n",
      " [  172     0    19    10     0    83     0     0     0     5     6     0\n",
      "    497     3    25     0     4     6     0]\n",
      " [   34     0     0     0     0   108     0     0     0     7    11     0\n",
      "      0  2591    28     0     0     2     0]\n",
      " [  411     0    67    21     0    61     0     1     0     0     2     0\n",
      "      0    14  6760     0     6    15     1]\n",
      " [    2     0     1     0     0   252     0     0     0     1    21     0\n",
      "      0    36     1   454     3     0     0]\n",
      " [   93     0     5     0     0   245     0     0     0     2     2     0\n",
      "      0    50     7     0  2079     9     0]\n",
      " [  389     0    18     2     3   166     0     0     0     1     1     0\n",
      "      1     8    10     0     6  2813     2]\n",
      " [ 1219     0   190    15     8   199     0     0     0     0     3     0\n",
      "      0    18   257     2     4    28  2056]]\n"
     ]
    }
   ],
   "source": [
    "#TEST WITH RANDOM PARAMETERS TO TEST FEATURES IMPORTANCE\n",
    "#July\n",
    "#split test and validation\n",
    "X_trainP,X_testP,y_trainP,y_testP  = train_test_split(X_train,y_train, test_size=0.25,random_state=5)#,stratify=y_classP)\n",
    "\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(bootstrap=0, criterion='gini', max_depth=None, max_features='sqrt', \n",
    "                             min_samples_leaf=4, min_samples_split=3, n_estimators=150, n_jobs=40)\n",
    "\n",
    "rfc.fit(X_trainP.values, y_trainP.values)\n",
    "\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "#pickle.dump(clf, open('RFmodel_1x1_scenario2_test', 'wb'))\n",
    "\n",
    "\n",
    "#accuracy\n",
    "y_test_pred=rfc.predict(X_testP)      \n",
    "y_test_pred_s=pd.Series(y_test_pred, dtype='float')\n",
    "        \n",
    "#to calculate accuracy, go back to array    \n",
    "accuracy = 100.0*(y_testP.array == y_test_pred_s.array).sum()/X_testP.shape[0]\n",
    "print('Accuracy is :' + str(round(accuracy,2)))\n",
    "\n",
    "confusion_mat=confusion_matrix(y_testP,y_test_pred_s)\n",
    "print (confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=0, class_weight=None, criterion='entropy',\n",
      "            max_depth=12, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=4, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "model = pickle.load(open('RFmodel_LUCAS_[2]_level2-crop_all-polygons_janv-jul2018','rb'))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Random Forest\n",
      "Elapsed time for training: 556.89 sec\n"
     ]
    }
   ],
   "source": [
    "#July\n",
    "t = time.time()\n",
    "Method = 'Random Forest'\n",
    "print('Method:', Method)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(bootstrap=0, criterion='entropy', max_depth=12, max_features='sqrt', \n",
    "                             min_samples_leaf=4, min_samples_split=3, n_estimators=150, n_jobs=40)\n",
    "\n",
    "rfc.fit(X_train.values, y_train.values)\n",
    "\n",
    "print('Elapsed time for training: %.02f sec' % (time.time() - t))\n",
    "#pickle.dump(clf, open('RFmodel_1x1_scenario2_test', 'wb'))\n",
    "\n",
    "import pickle\n",
    "pickle.dump(rfc, open('RFmodel_LUCAS_'+str(biome)+'_level2-crop_all-polygons_janv-jul2018bis', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_randomized_pipe.named_steps['clf'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all the models tried\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to import in GEE\n",
    "#index_column_VHVV=lucas.columns[76:148]\n",
    "#index_column_class=lucas.columns[1:4]\n",
    "#print(index_column_class)\n",
    "#print(index_column_VHVV)\n",
    "#create a csv with all the data\n",
    "#lucas_gee=lucas.loc[:,index_column_class.union(index_column_VHVV)]\n",
    "#lucas_gee.to_csv('lucas_crop_grass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform in byte\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler\n",
    "#data=X_features\n",
    "\n",
    "#scaler.data_max_=0\n",
    "#scaler.data_min_=-25\n",
    "\n",
    "#print(scaler.data_max_)\n",
    "#scaler.fit(lucas_test)\n",
    "#scaler.fit_transform(lucas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some test to avoid loading polygons csv\n",
    "#goupby by class and polygon - give count of pixels per polygons\n",
    "#test=pd_level2.groupby(['ClassL2','POINT_I'])['ClassL2','POINT_I'].count()\n",
    "#print(test.head())\n",
    "#how to count the number of polygon per class? level 0?\n",
    "#lucasgroup=pd_level2.groupby(['ClassL2','POINT_I'])['POINT_I'].size()\n",
    "#print(lucasgroup.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement with the biome class for the main crops we are interested\n",
    "#+the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
