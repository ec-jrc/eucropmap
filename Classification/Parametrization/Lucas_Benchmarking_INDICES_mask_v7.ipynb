{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EU crop map - Benchmarking on the time period for the mask classes\n",
    "## 1) Split by polygones - accuracy per pixels and per polygone\n",
    "## 2) Split by pixels - accuracy per pixels\n",
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JEODPP\n",
    "data_path='/eos/jeodpp/data/projects/REFOCUS/data/S1_GS/all-10days/Map_v7/'\n",
    "project_path='/eos/jeodpp/data/projects/REFOCUS/classification/'\n",
    "path_pol = '/eos/jeodpp/data/projects/REFOCUS/data/polygons/v7'\n",
    "results='/eos/jeodpp/data/projects/REFOCUS/classification/'\n",
    "\n",
    "local='/eos/jeodpp/home/users/verheas/data/LUCAS/v7/'\n",
    "\n",
    "#working directory\n",
    "pwd = project_path\n",
    "\n",
    "# !pip install matplotlib --user\n",
    "# !pip install sklearn --user\n",
    "#import \n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "#1) load the S1 10 days extracted values in GEE for all polygons\n",
    "\n",
    "pd_lucas= pd.read_csv(os.path.join(data_path,'S1_point_allV7_10days_10m_1Jan-31Dec_EU_ratio-db.csv'),dtype={'level_1':int,'level_2':int})\n",
    "print('pd_lucas',pd_lucas.shape)\n",
    "\n",
    "#concatenate all the data in one dataframe\n",
    "#group cropland, grassland and bareland \n",
    "#number of pixels per class\n",
    "print(pd_lucas.level_1.value_counts())\n",
    "print(pd_lucas.level_2.value_counts())\n",
    "pd_lucas.head()\n",
    "\n",
    "#number of pixels per class\n",
    "#pd_lucas.LC1_COD.value_counts()\n",
    "#pd_lucas.head()\n",
    "pd_lucas.columns\n",
    "\n",
    "##############1.2 Load the shapefile with the polygons - useful to split the polygons in training and test dataset for the accuracy ######################\n",
    "# load csv with of the polygons\n",
    "#2)load csv with the polygons for the split test/validation\n",
    "lucas_polygons = pd.read_csv(os.path.join(path_pol,'LUCAS_2018_Copernicus_attributes_cropmap_level1-2.csv'))\n",
    "lucas_polygons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################Parameters##################################################\n",
    "#classes - stored in a table 'legend-lucas-all'\n",
    "table_class=pd.read_csv(os.path.join(project_path,'table/legend-lucas-all-v7.csv'),dtype=pd.Int64Dtype())\n",
    "\n",
    "classes_L1=list(table_class['classes_L1'].dropna()) \n",
    "classes_L2=list(table_class['classes_L2'].dropna())\n",
    "\n",
    "#level\n",
    "level_1='level_1'\n",
    "level_2='level_2'\n",
    "\n",
    "##################################Parameters##################################################\n",
    "#classes - stored in a table 'legend-lucas-all'\n",
    "table_class=pd.read_csv(os.path.join(project_path,'table/legend-lucas-all-v2.csv'),dtype=pd.Int64Dtype())\n",
    "\n",
    "classes_L1=list(table_class['classes_L1'].dropna())\n",
    "classes_L2=list(table_class['classes_L2'].dropna())\n",
    "\n",
    "#remap classes and selection of classes to map Level 1\n",
    "classes_in_L1 =  list(table_class['classes_all'].dropna())\n",
    "\n",
    "classes_in_L2 = list(table_class['classes_all'].dropna()),\n",
    "\n",
    "#classes affected by biome selection\n",
    "classes_L1_B= list(table_class['classes_L1_B'].dropna())\n",
    "classes_L2_B= list(table_class['classes_L2'].dropna())\n",
    "\n",
    "#Classes non affected by biome selection\n",
    "#Classes from (A) Artificial, (F) Bare lands and (H) Wetlands can be considered in each models - no biome dependent\n",
    "classes_L1_NB=list(table_class['classes_L1_NB'].dropna())\n",
    "classes_L2_NB=[]\n",
    "#summary of the classes used in the classification\n",
    "classes_classif_L1= list(table_class['L1_BIOME'].dropna())\n",
    "classes_classif_L1_simplify=list(table_class['L1_B_harmon'].dropna())\n",
    "\n",
    "#[100,200,520,300,400,600,800]\n",
    "classes_classif_L2=list(table_class['L2_BIOME'].dropna())\n",
    "classes_classif_L2_simplify=list(table_class['L2_B_harmon'].dropna())\n",
    "\n",
    "###################################Choose parameters for this run #############################################\n",
    "#classes for the classification and biome/no biome differentiation if needed\n",
    "classes_B=classes_L1_B\n",
    "print ('classes_B',classes_B)\n",
    "\n",
    "classes_NB=classes_L1_NB\n",
    "print ('classes_NB',classes_NB)\n",
    "\n",
    "#level\n",
    "level=level_1\n",
    "print('level',level)\n",
    "#crop - level 2, from the table we load only the crop type classes\n",
    "classes=classes_L1\n",
    "print('level',classes)\n",
    "\n",
    "#Split for the train/test dataset - we run it with all the polygons\n",
    "#split_test = 0\n",
    "\n",
    "#summary of the classses used for the classification\n",
    "classes_classif=classes_classif_L1\n",
    "print ('classes_classif',classes_classif)\n",
    "classes_classif_simplify=classes_classif_L1_simplify\n",
    "print ('classes_classif_simplify',classes_classif_simplify)\n",
    "###################################Labels of the classes #############################################\n",
    "labels_csv = pd.read_csv(os.path.join(project_path,'table/legend-lucas2.csv'))\n",
    "labels=labels_csv[labels_csv['class'].isin(classes)] # select only the used labels\n",
    "labels_s=labels_csv[labels_csv['class'].isin(classes)] # select only the used labels\n",
    "print(classes)\n",
    "print(labels)\n",
    "print(labels_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#### 2) Prepare the data for the classification ##############\n",
    "##############################################################\n",
    "\n",
    "#############2.1 Select level of work and classes\n",
    "#copy values in a new column 'Classif' that we will use in the rest of the script\n",
    "pd_lucas['Classif']=pd_lucas[level]\n",
    "print(pd_lucas.shape)\n",
    "\n",
    "#add the biome after the class\n",
    "pd_lucas_biome=pd_lucas[pd_lucas.Classif.isin(classes_B)]\n",
    "pd_lucas_nobiome=pd_lucas[pd_lucas.Classif.isin(classes_NB)]\n",
    "\n",
    "pd_lucas_biome['ClassifB']=pd_lucas_biome['Classif'].astype(str) + pd_lucas_biome['stratum'].astype(str)\n",
    "pd_lucas_nobiome['ClassifB']=pd_lucas_nobiome['Classif'].astype(str) + '0'\n",
    "\n",
    "pd_lucas_b=pd_lucas_biome.append(pd_lucas_nobiome)\n",
    "\n",
    "#legend level 1 - create new column and copy values\n",
    "#pd_level1['ClassL1B']=pd_level1[['LC1_COD', 'BIOME_N']].apply(lambda x: ''.join(x.map(str)), axis=1)\n",
    "#pd_level1['ClassL1B']=pd_level1['ClassL1'].astype(str) + pd_level1['BIOME_N'].astype(str)\n",
    "#print(pd_lucas_b.head())\n",
    "print(pd_lucas_b.ClassifB.value_counts())\n",
    "\n",
    "#############2.2 Prepare the dataframe with the pixels extraction\n",
    "\n",
    "lucas_polygons['Classif']=lucas_polygons.level_2\n",
    "\n",
    "#reclassify\n",
    "#lucas_polygons.Classif=lucas_polygons.Classif.replace(classes_in,\n",
    "#                                                        classes_remap)\n",
    "#print(lucas_polygons.shape)\n",
    "#print(lucas_polygons.Classif.value_counts())\n",
    "\n",
    "#select the classes of interest for Level 1\n",
    "#add the biome after the class\n",
    "\n",
    "lucas_polygons_biome=lucas_polygons[lucas_polygons.Classif.isin(classes_B)]\n",
    "lucas_polygons_nobiome=lucas_polygons[lucas_polygons.Classif.isin(classes_NB)]\n",
    "\n",
    "print(lucas_polygons_biome.shape)\n",
    "print(lucas_polygons_biome.Classif.value_counts())\n",
    "print(lucas_polygons_nobiome.shape)\n",
    "print(lucas_polygons_nobiome.Classif.value_counts())\n",
    "\n",
    "lucas_polygons_biome['ClassifB']=lucas_polygons_biome['Classif'].astype(str) + lucas_polygons_biome['stratum'].astype(str)\n",
    "lucas_polygons_nobiome['ClassifB']=lucas_polygons_nobiome['Classif'].astype(str) + '0'\n",
    "print(lucas_polygons_biome.ClassifB.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the benchmarking on the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd.read_csv( os.path.join(project_path,'table/RF-parameters-table-INDICE_v2.csv'))\n",
    "parameters['name']=parameters['NAME_indice']+parameters['NAME_date']\n",
    "parameters['regex']=parameters['REGEX_indice']+parameters['REGEX_time']\n",
    "print (parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Split on polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manip='INDICES-BIOME-STRATIFY-LEVEL1_pol'\n",
    "if not os.path.exists(os.path.join('result',manip)):\n",
    "    os.mkdir(os.path.join('result',manip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option when the biomes are separated and put back together\n",
    "for i_test in range(0,len(parameters['name'])):\n",
    "    print('processing : '+manip+'  ' +parameters['name'][i_test])\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2332']\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2194']\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2234']\n",
    "    \n",
    "    #subset by biomes and create another loop for the 4 biomes\n",
    "    #execute the split/train\n",
    "    #join the results and calculate the OA\n",
    "    y_test_s_all=pd.Series([])\n",
    "    y_test_pred_s_all=pd.Series([])\n",
    "    y_test_s_all_pol=pd.Series([])\n",
    "    y_test_pred_s_all_pol=pd.Series([])\n",
    "    \n",
    "    for biome in range(1,3):\n",
    "        print(biome)\n",
    "        # 1 / create a text file for log recording\n",
    "        file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_1_Processing_Log.txt'),\"w\") \n",
    "\n",
    "        file.write('Processing summary \\n') \n",
    "        file.write(\"Date and time start: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+\"\\n\") \n",
    "        file.write(\"Classes : \"+ str(classes)+\"\\n\") \n",
    "        file.write(\"Regex : \"+ str(parameters['regex'][i_test])+\"\\n\") \n",
    "        file.write(\"Name : \"+ str(parameters['name'][i_test])+\"\\n\") \n",
    "    \n",
    "        #select biome on the polygons\n",
    "        lucas_polygons_biome_b=lucas_polygons_biome[lucas_polygons_biome.stratum.isin([biome])]\n",
    "        lucas_polygons_b=lucas_polygons_biome_b.append(lucas_polygons_nobiome)\n",
    "        #drop 2143 as there is only one\n",
    "        #lucas_polygons_b = lucas_polygons_b[lucas_polygons_b.ClassifB != 2143]\n",
    "\n",
    "        #print('dataframe complet',lucas_polygons_b.shape)\n",
    "        #variety of classes per pixels for the selected biome\n",
    "        #print('dataframe complet',pd.value_counts(lucas_polygons_b.Classif,sort=True))\n",
    "        #print('dataframe complet',lucas_polygons_b.head())\n",
    "        print(lucas_polygons_b.Classif.value_counts())\n",
    "\n",
    "        # Subset the polygons\n",
    "        X_featuresP=lucas_polygons_b.filter(items=['POINT_ID','Classif'])\n",
    "        y_classP=lucas_polygons_b['Classif']#.astype(np.float32)\n",
    "        file.write(\"Input DB polygons shape  : \"+ str(X_featuresP.shape)+\"\\n\") \n",
    "        file.write(\"Input DB polygons columns  : \"+ str(list(X_featuresP.columns))+\"\\n\") \n",
    "    \n",
    "        # 1/ Split between test and train\n",
    "        #TO BE DONE ON THE LUCAS POLYGONS\n",
    "        #https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn\n",
    "        X_trainP,X_testP,y_trainP,y_testP  = train_test_split(X_featuresP,y_classP, test_size=0.2,random_state=5,stratify=y_classP)\n",
    "        file.write(\"X_trainP.shape  : \"+ str(X_trainP.shape)+\"\\n\") \n",
    "        file.write(\"X_testP.shape  : \"+ str(X_testP.shape)+\"\\n\")\n",
    "        file.write(\"y_trainP.shape  : \"+ str(y_trainP.shape)+\"\\n\")\n",
    "        file.write(\"y_testP.shape  : \"+ str(y_testP.shape)+\"\\n\")\n",
    "\n",
    "        # 2/select the pixels from the polygons\n",
    "        #and Subset the DB with regex\n",
    "        indexPOINItrain=pd_lucas_b['POINT_ID'].isin(X_trainP['POINT_ID'])\n",
    "        indexPOINItest=pd_lucas_b['POINT_ID'].isin(X_testP['POINT_ID'])\n",
    "        \n",
    "        X_train=pd_lucas_b[indexPOINItrain].filter(regex=parameters['regex'][i_test])\n",
    "        y_train=pd_lucas_b[indexPOINItrain]['Classif']\n",
    "        X_test=pd_lucas_b[indexPOINItest].filter(regex=parameters['regex'][i_test])\n",
    "        y_test=pd_lucas_b[indexPOINItest]['Classif']\n",
    "        \n",
    "        #write\n",
    "        file.write(\"Input DB X_train pixels shape  : \"+ str(X_train.shape)+\"\\n\") \n",
    "        file.write(\"Input DB X_train pixels columns  : \"+ str(list(X_train.columns))+\"\\n\") \n",
    "        \n",
    "        #keep all info to aggregate prediction per polygons\n",
    "        y_train_pol=pd_lucas_b[indexPOINItrain]\n",
    "        y_test_pol=pd_lucas_b[indexPOINItest]\n",
    "\n",
    "       \n",
    "        # 4/ Save the class distribution for training and testing as CSV\n",
    "        #x = pd.DataFrame(y_train.value_counts().rename_axis('class').reset_index(name='counts'))\n",
    "        x = pd.DataFrame({\"count_pol\": y_train_pol.groupby('POINT_ID').apply(max)['Classif'].value_counts(), \"count_pixel\": y_train.value_counts()}).rename_axis('class')\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_2_Training_class_count_polygons.csv'))\n",
    "        #x = pd.DataFrame(y_test.value_counts().rename_axis('class').reset_index(name='counts'))\n",
    "        x = pd.DataFrame({\"count_pol\": y_test_pol.groupby('POINT_ID').apply(max)['Classif'].value_counts(), \"count_pixel\": y_test.value_counts()}).rename_axis('class')\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_3_Testing_class_count_polygons.csv'))\n",
    "\n",
    "\n",
    "        # 5/ Fit the RANDOM PARAMETERS T\n",
    "        t = time.time()    \n",
    "        clf = RandomForestClassifier(bootstrap=0, criterion='gini', max_depth=None, max_features='auto', \n",
    "                                     min_samples_leaf=12, min_samples_split=3, n_estimators=800, n_jobs=40)\n",
    "                                                                                                                                                                                    \n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        training_time=time.time() - t\n",
    "        file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_1_Processing_Log.txt'),\"a\") \n",
    "        file.write(\"Elapsed time for training  : \"+ str(round(training_time))+\" sec \\n\")\n",
    "        #file.write(\"Model  : \" +str(clf)+\"\\n\")\n",
    "        file.close()\n",
    "\n",
    "        # 6/ Feature importances as  CSV\n",
    "        x = list(zip(clf.feature_importances_,X_train.columns))\n",
    "        x = pd.DataFrame(x,columns=[\"Importance\",\"Feature_Name\"])\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_2_Feature_importance.csv') )\n",
    "        \n",
    "        #append the test value in a file for the 4 biomes\n",
    "        # 7/ OA -evaluate accuracy with the test dataset for the unique rf model\n",
    "        #reclassify the classes by biomes to the simple classes \n",
    "        #transform to series to use replace\n",
    "\n",
    "        #Prediction\n",
    "        y_test_pred=clf.predict(X_test)      \n",
    "        y_test_s=pd.Series(y_test, dtype='float')\n",
    "        #y_test_s=y_test_s.replace(classes_classif,classes_classif_simplify)\n",
    "        \n",
    "        y_test_pred_s=pd.Series(y_test_pred, dtype='float')\n",
    "        #y_test_pred_s=y_test_pred_s.replace(classes_classif,classes_classif_simplify)\n",
    "                \n",
    "        #to calculate accuracy, go back to array    \n",
    "        accuracy = 100.0*(y_test_s.array == y_test_pred_s.array).sum()/y_test_s.shape[0]\n",
    "        print('Accuracy is :' + str(round(accuracy,2)))\n",
    "    \n",
    "        #del(file)\n",
    "        file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_1_1_Accuracy.txt'),\"w\") \n",
    "        #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "        file1.write(str(accuracy)+\"\\n\") \n",
    "        file1.close()\n",
    "        \n",
    "        # 8/ Classification report\n",
    "        report = classification_report(y_test_s, y_test_pred_s, output_dict=True)\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_3_classification_report.csv') )\n",
    "        \n",
    "        # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "        confusion_mat=confusion_matrix(y_test_s,y_test_pred_s,labels=classes)\n",
    "        confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "        confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_4_confusion_matrix_class.csv'))\n",
    "        \n",
    "        #accuracy mode polygon\n",
    "        #apply a majority rule (mode)\n",
    "        #group it with y_test\n",
    "        y_test_results=pd.DataFrame({'POINT_ID':y_test_pol['POINT_ID'],'ref':y_test,'predict':y_test_pred})\n",
    "        y_test_results=y_test_results.groupby(['POINT_ID'])['predict','ref'].agg(lambda x: x.mode()[0])\n",
    "        \n",
    "        #to calculate accuracy, go back to array    \n",
    "        accuracy_pol = 100.0*(y_test_results['ref'].array == y_test_results['predict'].array).sum()/y_test_results.shape[0]\n",
    "        print('Accuracy is :' + str(round(accuracy_pol,2)))\n",
    "        \n",
    "        #del(file)\n",
    "        file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_1_1_Accuracy_pol.txt'),\"w\") \n",
    "        #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "        file1.write(str(accuracy_pol)+\"\\n\") \n",
    "        file1.close()\n",
    "        # 8/ Classification report\n",
    "        report = classification_report(y_test_results['ref'],y_test_results['predict'], output_dict=True)\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_3_classification_report_pol.csv') )\n",
    "        \n",
    "        # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "        confusion_mat=confusion_matrix(y_test_results['ref'],y_test_results['predict'],labels=classes)\n",
    "        confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "        confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_4_confusion_matrix_class_pol.csv'))\n",
    "        \n",
    "        y_test_s_all=y_test_s_all.append(y_test_s)      \n",
    "        #print(y_test_all)        \n",
    "        y_test_pred_s_all=y_test_pred_s_all.append(y_test_pred_s)\n",
    "        \n",
    "        #pol\n",
    "        y_test_s_all_pol=y_test_s_all_pol.append(y_test_results['ref'])      \n",
    "        #print(y_test_all)        \n",
    "        y_test_pred_s_all_pol=y_test_pred_s_all_pol.append(y_test_results['predict'])\n",
    "        \n",
    "    #to calculate accuracy, go back to array    \n",
    "    accuracy = 100.0*(y_test_s_all.array == y_test_pred_s_all.array).sum()/y_test_s_all.shape[0]\n",
    "    print('Accuracy is :' + str(round(accuracy,2)))\n",
    "   \n",
    "    #del(file)\n",
    "    file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Accuracy.txt'),\"w\") \n",
    "    #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "    file1.write(str(accuracy)+\"\\n\") \n",
    "    file1.close()\n",
    "    \n",
    "    # 8/ Classification report\n",
    "    report = classification_report(y_test_s_all, y_test_pred_s_all, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_3_classification_report.csv') )\n",
    "\n",
    "    # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "    confusion_mat=confusion_matrix(y_test_s_all,y_test_pred_s_all,labels=classes)\n",
    "    confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "    confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_4_confusion_matrix_class.csv'))\n",
    "    #confusion_mat_class=pd.DataFrame(confusion_mat,index= list(labels_s['class']),columns=list(labels_s['class']))\n",
    "    #confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_remap_4_confusion_matrix_class.csv') )\n",
    "    #confusion_mat_label=pd.DataFrame(confusion_mat,index= list(labels_s['label']),columns=list(labels_s['label']))\n",
    "    #confusion_mat_label.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_remap_4_confusion_matrix_label.csv') )\n",
    "\n",
    "    #to calculate accuracy, go back to array    \n",
    "    accuracy_pol = 100.0*(y_test_s_all_pol.array == y_test_pred_s_all_pol.array).sum()/y_test_s_all_pol.shape[0]\n",
    "    print('Accuracy is :' + str(round(accuracy_pol,2)))\n",
    "    \n",
    "    #del(file)\n",
    "    file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Accuracy_pol.txt'),\"w\") \n",
    "    #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "    file1.write(str(accuracy_pol)+\"\\n\") \n",
    "    file1.close()\n",
    "    \n",
    "    # 8/ Classification report\n",
    "    report = classification_report(y_test_s_all_pol, y_test_pred_s_all_pol, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_3_classification_report_pol.csv') )\n",
    "\n",
    "    # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "    confusion_mat=confusion_matrix(y_test_s_all_pol,y_test_pred_s_all_pol,labels=classes)\n",
    "    confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "    confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_4_confusion_matrix_class_pol.csv'))\n",
    "    \n",
    "    file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Processing_Log.txt'),\"a\") \n",
    "    file.write(\"Date and time end: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+\"\\n\") \n",
    "    file.close()\n",
    "    \n",
    "    del(df,clf,confusion_mat)#confusion_mat_label,confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Split on pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manip='INDICES-BIOME-STRATIFY-LEVEL1_pix'\n",
    "if not os.path.exists(os.path.join('result',manip)):\n",
    "    os.mkdir(os.path.join('result',manip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option when the biomes are separated and put back together\n",
    "for i_test in range(0,len(parameters['name'])):\n",
    "    print('processing : '+manip+'  ' +parameters['name'][i_test])\n",
    "    lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2332']\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2194']\n",
    "    #lucas_polygons_biome=lucas_polygons_biome[lucas_polygons_biome.ClassifB!='2234']\n",
    "    \n",
    "    #subset by biomes and create another loop for the 4 biomes\n",
    "    #execute the split/train\n",
    "    #join the results and calculate the OA\n",
    "    y_test_s_all=pd.Series([])\n",
    "    y_test_pred_s_all=pd.Series([])\n",
    "\n",
    "    \n",
    "    for biome in range(1,3):\n",
    "        print(biome)\n",
    "        # 1 / create a text file for log recording\n",
    "        file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_1_Processing_Log.txt'),\"w\") \n",
    "\n",
    "        file.write('Processing summary \\n') \n",
    "        file.write(\"Date and time start: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+\"\\n\") \n",
    "        file.write(\"Classes : \"+ str(classes)+\"\\n\") \n",
    "        file.write(\"Regex : \"+ str(parameters['regex'][i_test])+\"\\n\") \n",
    "        file.write(\"Name : \"+ str(parameters['name'][i_test])+\"\\n\") \n",
    "    \n",
    "        #select biome on the polygons\n",
    "        lucas_polygons_biome_b=lucas_polygons_biome[lucas_polygons_biome.stratum.isin([biome])]\n",
    "        lucas_polygons_b=lucas_polygons_biome_b.append(lucas_polygons_nobiome)\n",
    "        #drop 2143 as there is only one\n",
    "        #lucas_polygons_b = lucas_polygons_b[lucas_polygons_b.ClassifB != 2143]\n",
    "\n",
    "        #print('dataframe complet',lucas_polygons_b.shape)\n",
    "        #variety of classes per pixels for the selected biome\n",
    "        #print('dataframe complet',pd.value_counts(lucas_polygons_b.Classif,sort=True))\n",
    "        #print('dataframe complet',lucas_polygons_b.head())\n",
    "        print(lucas_polygons_b.Classif.value_counts())\n",
    "\n",
    "        # Subset the polygons\n",
    "        X_features=pd_lucas_b.filter(regex=parameters['regex'][i_test])\n",
    "        y_class=pd_lucas_b['Classif']#.astype(np.float32)\n",
    "        file.write(\"Input DB pixel shape  : \"+ str(X_features.shape)+\"\\n\") \n",
    "        file.write(\"Input DB pixel columns  : \"+ str(list(X_features.columns))+\"\\n\") \n",
    "    \n",
    "        # 1/ Split between test and train\n",
    "        #TO BE DONE ON THE LUCAS POLYGONS\n",
    "        #https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn\n",
    "        X_train,X_test,y_train,y_test  = train_test_split(X_features,y_class, test_size=0.2,random_state=5,stratify=y_class)\n",
    "        \n",
    "        file.write(\"X_train.shape  : \"+ str(X_train.shape)+\"\\n\") \n",
    "        file.write(\"X_test.shape  : \"+ str(X_test.shape)+\"\\n\")\n",
    "        file.write(\"y_train.shape  : \"+ str(y_train.shape)+\"\\n\")\n",
    "        file.write(\"y_test.shape  : \"+ str(y_test.shape)+\"\\n\")\n",
    "\n",
    "        # 2/select the pixels from the polygons\n",
    "        #and Subset the DB with regex\n",
    "             \n",
    "        #write\n",
    "        file.write(\"Input DB X_train pixels shape  : \"+ str(X_train.shape)+\"\\n\") \n",
    "        file.write(\"Input DB X_train pixels columns  : \"+ str(list(X_train.columns))+\"\\n\") \n",
    "        \n",
    "      \n",
    "        # 4/ Save the class distribution for training and testing as CSV\n",
    "        #x = pd.DataFrame(y_train.value_counts().rename_axis('class').reset_index(name='counts'))\n",
    "        x = pd.DataFrame({\"count_pixel\": y_train.value_counts()}).rename_axis('class')\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_2_Training_class_count_pixels.csv'))\n",
    "        #x = pd.DataFrame(y_test.value_counts().rename_axis('class').reset_index(name='counts'))\n",
    "        x = pd.DataFrame({\"count_pixel\": y_test.value_counts()}).rename_axis('class')\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_3_Testing_class_count_pixels.csv'))\n",
    "\n",
    "\n",
    "        # 5/ Fit the RANDOM PARAMETERS T\n",
    "        t = time.time()    \n",
    "        clf = RandomForestClassifier(bootstrap=0, criterion='gini', max_depth=None, max_features='auto', \n",
    "                                     min_samples_leaf=12, min_samples_split=3, n_estimators=800, n_jobs=40)\n",
    "                                                                                                                                                                                    \n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        training_time=time.time() - t\n",
    "        file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_1_1_Processing_Log.txt'),\"a\") \n",
    "        file.write(\"Elapsed time for training  : \"+ str(round(training_time))+\" sec \\n\")\n",
    "        #file.write(\"Model  : \" +str(clf)+\"\\n\")\n",
    "        file.close()\n",
    "\n",
    "        # 6/ Feature importances as  CSV\n",
    "        x = list(zip(clf.feature_importances_,X_train.columns))\n",
    "        x = pd.DataFrame(x,columns=[\"Importance\",\"Feature_Name\"])\n",
    "        x.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_'+str(biome)+'_remap_2_Feature_importance.csv') )\n",
    "        \n",
    "        #append the test value in a file for the 4 biomes\n",
    "        # 7/ OA -evaluate accuracy with the test dataset for the unique rf model\n",
    "        #reclassify the classes by biomes to the simple classes \n",
    "        #transform to series to use replace\n",
    "\n",
    "        #Prediction\n",
    "        y_test_pred=clf.predict(X_test)      \n",
    "        y_test_s=pd.Series(y_test, dtype='float')\n",
    "        #y_test_s=y_test_s.replace(classes_classif,classes_classif_simplify)\n",
    "        \n",
    "        y_test_pred_s=pd.Series(y_test_pred, dtype='float')\n",
    "        #y_test_pred_s=y_test_pred_s.replace(classes_classif,classes_classif_simplify)\n",
    "                \n",
    "        #to calculate accuracy, go back to array    \n",
    "        accuracy = 100.0*(y_test_s.array == y_test_pred_s.array).sum()/y_test_s.shape[0]\n",
    "        print('Accuracy is :' + str(round(accuracy,2)))\n",
    "    \n",
    "        #del(file)\n",
    "        file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_1_1_Accuracy.txt'),\"w\") \n",
    "        #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "        file1.write(str(accuracy)+\"\\n\") \n",
    "        file1.close()\n",
    "        \n",
    "        # 8/ Classification report\n",
    "        report = classification_report(y_test_s, y_test_pred_s, output_dict=True)\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_3_classification_report.csv') )\n",
    "        \n",
    "        # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "        confusion_mat=confusion_matrix(y_test_s,y_test_pred_s,labels=classes)\n",
    "        confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "        confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'biome'+str(biome)+'_4_confusion_matrix_class.csv'))\n",
    "        \n",
    "        \n",
    "        y_test_s_all=y_test_s_all.append(y_test_s)      \n",
    "        #print(y_test_all)        \n",
    "        y_test_pred_s_all=y_test_pred_s_all.append(y_test_pred_s)\n",
    "    \n",
    "        \n",
    "    #to calculate accuracy, go back to array    \n",
    "    accuracy = 100.0*(y_test_s_all.array == y_test_pred_s_all.array).sum()/y_test_s_all.shape[0]\n",
    "    print('Accuracy is :' + str(round(accuracy,2)))\n",
    "   \n",
    "    #del(file)\n",
    "    file1 = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Accuracy.txt'),\"w\") \n",
    "    #file.write(\"Accuracy of the classifier  : \" +str(round(accuracy,2))+\" % \"+\" \\n\")\n",
    "    file1.write(str(accuracy)+\"\\n\") \n",
    "    file1.close()\n",
    "    \n",
    "    # 8/ Classification report\n",
    "    report = classification_report(y_test_s_all, y_test_pred_s_all, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_3_classification_report.csv') )\n",
    "\n",
    "    # 9 / Extract confusion matrix to CSV - to fix - labels not correct\n",
    "    confusion_mat=confusion_matrix(y_test_s_all,y_test_pred_s_all,labels=classes)\n",
    "    confusion_mat_class=pd.DataFrame(confusion_mat,index=classes,columns=classes)\n",
    "    confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_4_confusion_matrix_class.csv'))\n",
    "    #confusion_mat_class=pd.DataFrame(confusion_mat,index= list(labels_s['class']),columns=list(labels_s['class']))\n",
    "    #confusion_mat_class.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_remap_4_confusion_matrix_class.csv') )\n",
    "    #confusion_mat_label=pd.DataFrame(confusion_mat,index= list(labels_s['label']),columns=list(labels_s['label']))\n",
    "    #confusion_mat_label.to_csv(os.path.join(local,'result',manip,parameters['name'][i_test]+'_remap_4_confusion_matrix_label.csv') )\n",
    "\n",
    "    file = open(os.path.join(local,'result',manip,parameters['name'][i_test]+'_regroup_remap_1_1_Processing_Log.txt'),\"a\") \n",
    "    file.write(\"Date and time end: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+\"\\n\") \n",
    "    file.close()\n",
    "    \n",
    "    del(df,clf,confusion_mat)#confusion_mat_label,confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
